{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> \n",
    "  <div style=\"padding:20px; \n",
    "              color:#1DA1F2; \n",
    "              font-size:220%; \n",
    "              border-radius:20px; \n",
    "              border-width: 5px; \n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 2px 2px 10px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 15px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: 1000px; \n",
    "              margin: auto;\">\n",
    "  Problem Statement\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's digital age, social media platforms are very much popular amongst all age categories. Approximately, 63.9% of the words population (5.24 billion) uses social media platforms with an average user spending roughly 2 hours 21 minutes on it daily. Today, social media platforms have become the primary channel for communication and interactions. However, since 2013 there has been a steady rise on the overall negativity present on the platforms. The main driving force for this rise is due to the hate speech and offensive language remarks present all over social media. These forms of expression can perpetuate discrimination, foster hostility, and negatively impact ones mental well-being.\n",
    "\n",
    "Hate speech is generally defined as any communication that attacks or denigrates a person or group based on attributes like race, religion, ethnicity, gender, sexual orientation, or disability. Offensive language, less severe, is defined by words or phrases that can be used in a disrespectful, insulting, or hurtful manner without targeting a particular group. Both are considered as a negative form of message to the opposite party.\n",
    "\n",
    "The presence of such language leads to several significant issues:\n",
    "\n",
    "User Safety: Hate speech and offensive language create unsafe online spaces, discouraging healthy discussions and making users feel alienated.\n",
    "Platform Reputation: Platforms that fail to address these concerns risk losing user trust and facing scrutiny from regulators.\n",
    "Legal and Ethical Implications: Many countries enforce strict laws against hate speech, requiring platforms to take proactive measures to filter such content.\n",
    "Scalability of Moderation: Given the vast amount of content generated daily, manual moderation is neither practical nor scalable.\n",
    "\n",
    "Our goal is to build a model which can identify such negative remarks and comments and prevent it from reaching the victim before its too late. This way we can make social media a safe environment for everyone to use and foster healthy relationships between the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"image.webp\" alt=\"Image Description\" width=\"550\" style=\"border-radius: 20px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  <img src=\"image2.jpg\" alt=\"Image Description\" width=\"600\" style=\"border-radius: 20px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <div style=\"padding:20px; \n",
    "              color:#1DA1F2; \n",
    "              font-size:220%; \n",
    "              border-radius:20px; \n",
    "              border-width: 5px; \n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 2px 2px 10px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 15px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: 1000px; \n",
    "              margin: auto;\">\n",
    "  Background information on the dataset\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using, named hate_speech_offensive, is a meticulously curated collection of annotated tweets with the specific purpose of detecting hate speech and offensive language. The dataset primarily consists of English tweets and is designed to train machine learning models or algorithms in the task of hate speech detection. It should be noted that the dataset has not been divided into multiple subsets, and only the train split is currently available for use.\n",
    "\n",
    "The dataset includes several columns that provide valuable information for understanding each tweet's classification. The column count represents the total number of annotations provided for each tweet, whereas hate_speech_count signifies how many annotations classified a particular tweet as hate speech. On the other hand, offensive_language_count indicates the number of annotations categorizing a tweet as containing offensive language. Additionally, neither_count denotes how many annotations identified a tweet as neither hate speech nor offensive language.\n",
    "\n",
    "For researchers and developers aiming to create effective models or algorithms capable of detecting hate speech and offensive language on Twitter, this comprehensive dataset offers a rich resource for training and evaluation purposes.\n",
    "\n",
    "It consists of annotated tweets with information about their classification as hate speech, offensive language, or neither. Each row represents a tweet along with the corresponding annotations provided by multiple annotators. The main columns that will be essential for your analysis are: count (total number of annotations), hate_speech_count (number of annotations classifying a tweet as hate speech), offensive_language_count (number of annotations classifying a tweet as offensive language), neither_count (number of annotations classifying a tweet as neither hate speech nor offensive language).\n",
    "\n",
    "The data collection methodology used to create this dataset involved obtaining tweets from Twitter's public API using specific search terms related to hate speech and offensive language. These tweets were then manually labeled by multiple annotators who reviewed them for classification purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <div style=\"padding:20px; \n",
    "              color:#1DA1F2; \n",
    "              font-size:220%; \n",
    "              border-radius:20px; \n",
    "              border-width: 5px; \n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 2px 2px 10px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 15px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: 1000px; \n",
    "              margin: auto;\">\n",
    "  Preamble for this notebook\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"worried.png\" alt=\"Image Description\" width=\"400\" style=\"border-radius: 10px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  \n",
    "  <div style=\"margin-left: auto;\">\n",
    "This notebook focuses on <strong>sentiment analysis and hate speech detection</strong> using the <strong>Twitter Hate Speech Dataset</strong>. The primary objective is to <strong>analyze, classify, and evaluate textual data</strong> to identify offensive, abusive, or hateful content through various <strong>Natural Language Processing (NLP)</strong> techniques.\n",
    "\n",
    "<br>\n",
    "\n",
    "Due to the nature of the dataset, there are instances where <strong>vulgarities, offensive language, and hate speech</strong> are present throughout the notebook. These terms are strictly used <strong>for research, analysis, and educational purposes only</strong> — without any intention to promote or encourage hate speech.\n",
    "<br>\n",
    "\n",
    "The usage of such language is solely for the purpose of:\n",
    "- Understanding sentiment polarity in texts.\n",
    "- Evaluating the effectiveness of machine learning models in detecting hate speech.\n",
    "- Demonstrating how sentiment scores and feature selection techniques affect model performance.\n",
    "<br>\n",
    "\n",
    "If you feel uncomfortable with offensive language, <strong>viewer discretion is advised</strong>.\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <div style=\"padding:20px; \n",
    "              color:#1DA1F2; \n",
    "              font-size:220%; \n",
    "              border-radius:20px; \n",
    "              border-width: 5px; \n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 2px 2px 10px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 15px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: 1000px; \n",
    "              margin: auto;\">\n",
    "  User Manual\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [**Overview**](#overview)  \n",
    "   A brief introduction to significance & objectives of the user manual.\n",
    "\n",
    "2. [**Environment Setup**](#environment-setup)  \n",
    "   Step-by-step instructions for installing and configuring necessary dependencies.\n",
    "\n",
    "3. [**Needed dependencies**](#needed-dependencies)  \n",
    "   All dependencies needed to run the project\n",
    "\n",
    "4. [**Data Processing**](#data-processing)  \n",
    "   How to acquire, clean, and transform data for use in the project.\n",
    "\n",
    "5. [**Feature Engineering**](#feature-engineering)  \n",
    "    Steps taken to value add to the modelling phase of the project.\n",
    "\n",
    "6. [**Training & Testing**](#training-testing)  \n",
    "   Detailed steps to train the best model and evaluate its performance.\n",
    "\n",
    "7. [**Troubleshooting & FAQ**](#troubleshooting-faq)  \n",
    "   Common issues, solutions, and frequently asked questions.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "This manual provides a step-by-step guide to set up, run, and validate all the components required for this project. By following the instructions detailed here, users will be able to reproduce the final results—whether it’s training a model, running data processing pipelines, or deploying the application.\n",
    "\n",
    "### Objectives\n",
    "- **Setup Guidance:** Instructions on installing dependencies, configuring the environment, and preparing any required data or services.\n",
    "- **Data Processing and Model Training:** Clear steps to process raw data, train the model, and evaluate performance metrics.\n",
    "- **Reproducibility:** Detailed commands and configurations so users can replicate the exact results obtained by the authors.\n",
    "- **Troubleshooting and Best Practices:** Common issues, solutions, and recommended workflows to ensure a smooth experience.\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "1. **Create & Activate Your Virtual Environment (Optional in VS Code)**  \n",
    "   - From your terminal:\n",
    "     ```bash\n",
    "     python3 -m venv env\n",
    "     source env/bin/activate  # or env\\Scripts\\activate on Windows\n",
    "     ```\n",
    "   - Install your project dependencies inside this virtual environment:\n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```\n",
    "\n",
    "2. **Select the Interpreter in VS Code**  \n",
    "   - **Open Command Palette:**  \n",
    "     - Press <kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>P</kbd> (Windows/Linux) or <kbd>Cmd</kbd>+<kbd>Shift</kbd>+<kbd>P</kbd> (macOS).\n",
    "   - **Search for “Python: Select Interpreter”:**  \n",
    "     - Choose the virtual environment you created (`env` or another name).  \n",
    "   - VS Code will automatically use the Python version and packages from your selected environment.\n",
    "\n",
    "3. **Using Jupyter Notebooks in VS Code**  \n",
    "   - When you open or create a `.ipynb` file, you’ll see a kernel selector at the top (e.g., “Python 3.x”).\n",
    "   - Click on it and select the virtual environment you created.  \n",
    "   - VS Code will handle the rest, so all notebook cells will run using your virtual environment.\n",
    "\n",
    "4. **Verification**  \n",
    "   - Create a test file (or a notebook cell) to confirm you’re using the correct environment:\n",
    "     ```python\n",
    "     import sys\n",
    "     import os\n",
    "\n",
    "     print(\"Python executable:\", sys.executable)\n",
    "     print(\"Python version:\", sys.version)\n",
    "     print(\"Environment variable:\", os.getenv(\"YOUR_ENV_VAR\", \"Not set\"))\n",
    "     ```\n",
    "   - Running this will show the path to your Python executable, the Python version, and any environment variables you’ve set, confirming that you’re using the correct virtual environment.\n",
    "     ```\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "## Needed Dependencies\n",
    "\n",
    "- **To run this project make sure to import all the required libraries in any python cell within the notebook (Advised to do it as the first cell):**  \n",
    "\n",
    "\n",
    "    ```python\n",
    "\n",
    "    # Machine Learning Packages\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "    # Natural Language Processing (NLP) Libraries\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "    import re\n",
    "\n",
    "    # Visualization Libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "    from IPython.display import display_html\n",
    "    from IPython.display import display, HTML\n",
    "    from sklearn.tree import plot_tree\n",
    "    import plotly.express as px\n",
    "\n",
    "    # Data Manipulation and Processing\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from tabulate import tabulate\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "    import time\n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "    from pympler import asizeof\n",
    "    from imblearn.pipeline import Pipeline\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    import warnings\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    ```\n",
    "---\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "- **Preprocessing Steps:**  \n",
    "  - [Input steps taken...]\n",
    "  - [Input steps taken...]\n",
    "  - [Input steps taken...]\n",
    "  \n",
    "    ```python\n",
    "    df = pd.read_csv('train.csv')\n",
    "\n",
    "    class_labels = {0: 'Hate Speech', 1: 'Offensive Language', 2: 'Neither'}\n",
    "    class_counts = df['class'].value_counts()\n",
    "    df['labels'] = df['class'].map(class_labels)\n",
    "\n",
    "    df.isnull().sum()\n",
    "\n",
    "    df['class'] = df['class'].apply(lambda x: 0 if x in [0, 1] else 1)\n",
    "\n",
    "    class_labels = {0: 'negative', 1: 'positive'}\n",
    "    df['labels'] = df['class'].map(class_labels)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.remove('not')  # Keeping 'not' as it is important in negation (Can change the meaning of the sentence)\n",
    "\n",
    "    def clean(text):\n",
    "        # Remove 'RT' (Retweet indicator)\n",
    "        text = re.sub(r'\\bRT\\b', '', text)\n",
    "        # Remove URLs \n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        # Remove Twitter handles \n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        # Remove non-alphabetic characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize the text\n",
    "        words = text.split()\n",
    "        # Removing the stop words\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        # Rejoin the words back into a single string\n",
    "        cleaned_text = ' '.join(words)\n",
    "        return cleaned_text\n",
    "\n",
    "    df.loc[:, 'cleaned_tweet'] = df['tweet'].apply(clean)\n",
    "\n",
    "    tweet_length = df['cleaned_tweet'].apply(len)\n",
    "    df['tweet_length'] = df['cleaned_tweet'].apply(len)\n",
    "    Q1 = tweet_length.quantile(0.25)\n",
    "    Q3 = tweet_length.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df = df[(tweet_length >= lower_bound) & (tweet_length <= upper_bound)]\n",
    "\n",
    "    df.drop(['tweet', 'tweet_length'], axis=1, inplace=True)\n",
    "\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    df.to_csv('cleaned_dataset.csv', index=False)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "### **Features added:**\n",
    "\n",
    "1. **Lemmatization** \n",
    "  - Instead of translating the words to the base form, added the new words to the setence allowing both base word and actual word for an extended vocabulary set.\n",
    "\n",
    "2. **Undersampling** \n",
    "  - Undersampled the majority class to match the minority class count to ensure a fair training process during the modelling phase.\n",
    "\n",
    "3. **Sentiment score** \n",
    "  - Used VADER to compute the sentiment scores for the different tweets, which would be valuable additional information for the models during modelling phase.\n",
    "\n",
    "\n",
    "    ```python\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized_tokens = []\n",
    "        for token in tokens:\n",
    "            lemma = lemmatizer.lemmatize(token, get_wordnet_pos(token))\n",
    "            lemmatized_tokens.append(token)  \n",
    "            if lemma != token:\n",
    "                lemmatized_tokens.append(lemma)  \n",
    "        return \" \".join(lemmatized_tokens)\n",
    "\n",
    "    final_df = pd.read_csv('cleaned_dataset.csv')\n",
    "    final_df = final_df.dropna()\n",
    "    final_df['cleaned_tweet'] = final_df['cleaned_tweet'].apply(lemmatize_text) \n",
    "\n",
    "    df_majority=final_df[(final_df['class']==0)] \n",
    "    df_minority=final_df[(final_df['class']==1)] \n",
    "    df_majority_undersample=resample(df_majority, replace=False, n_samples=df_minority.shape[0], random_state=42)\n",
    "    df_balanced = pd.concat([df_majority_undersample, df_minority])\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    def get_sentiment(text):\n",
    "        sentiment = sia.polarity_scores(text)\n",
    "        return sentiment['compound']\n",
    "\n",
    "    sia_booster = sia.constants.BOOSTER_DICT\n",
    "    if 'fucking' in sia_booster:\n",
    "        sia_booster.pop('fucking')\n",
    "        print(\"Word 'fucking' DELETED from VADER\")\n",
    "    sia.lexicon['fucking'] = -2.0\n",
    "\n",
    "    df_balanced['sentiment_score'] = df_balanced['cleaned_tweet'].apply(get_sentiment)\n",
    "    df_balanced.drop(['count', 'hate_speech_count', 'offensive_language_count', 'neither_count'], axis=1, inplace=True)\n",
    "    df_balanced = df_balanced[df_balanced['cleaned_tweet'].str.strip() != '']\n",
    "    df_balanced.to_csv('train_balanced.csv', index=False)\n",
    "    \n",
    "    ```\n",
    "---\n",
    "\n",
    "## Training & Testing\n",
    "\n",
    "### **Training Steps:**\n",
    "\n",
    "1. **Data Preprocessing**  \n",
    "   - Load dataset (`train_balanced.csv`) into a Pandas DataFrame (`df_balanced`).\n",
    "   - Apply `adjust_word_scores` to compute sentiment-adjusted word scores with reference to the sentence sentiment score.\n",
    "\n",
    "2. **Feature Extraction**  \n",
    "   - Convert cleaned tweets into TF-IDF vectors using `TfidfVectorizer`.\n",
    "   - Store the transformed TF-IDF matrix as a sparse matrix (`X`).\n",
    "\n",
    "3. **Sentiment-Aware Feature Fusion**  \n",
    "   - Iterate through each word in the TF-IDF representation and adjust its score based on sentiment-adjusted word scores.\n",
    "   - Modify the term frequency values by multiplying with `(1 + adjusted sentiment score)`.\n",
    "\n",
    "4. **Train-Test Split**  \n",
    "   - Split the dataset into training (`X_train`, `y_train`) and testing (`X_test`, `y_test`) sets (80-20 split).\n",
    "\n",
    "5. **Model Training**  \n",
    "   - Train a `RandomForestClassifier` with:\n",
    "     - `n_estimators=300`\n",
    "     - `max_depth=None`\n",
    "     - `min_samples_leaf=1`\n",
    "     - `min_samples_split=2`\n",
    "   - Fit the model on the training data.\n",
    "\n",
    "6. **Model Evaluation**  \n",
    "   - Compute and display training and test accuracy.\n",
    "\n",
    "### **Testing Steps:**\n",
    "\n",
    "1. **Prepare Test Data**  \n",
    "   - Create a test dataset (`df_test`) with example sentences.\n",
    "   - Compute adjusted word scores using `adjust_word_scores`.\n",
    "\n",
    "2. **Vectorize Test Data**  \n",
    "   - Convert test sentences into TF-IDF vectors using the pre-trained `TfidfVectorizer`.\n",
    "\n",
    "3. **Apply Sentiment-Aware Adjustments**  \n",
    "   - Modify the TF-IDF values by incorporating sentiment-adjusted word scores.\n",
    "\n",
    "4. **Make Predictions**  \n",
    "   - Use the trained `RandomForestClassifier` to predict sentiment classes.\n",
    "   - Display predicted sentiment labels for test sentences.\n",
    "\n",
    "\n",
    "  ```python\n",
    "\n",
    "  def adjust_word_scores(sentence):\n",
    "      words = sentence.lower().split()\n",
    "      sentence_score = sia.polarity_scores(sentence)['compound']\n",
    "      word_scores = [sia.polarity_scores(w)['compound'] for w in words]\n",
    "      \n",
    "      adjusted_scores = []\n",
    "      for word_score in word_scores:\n",
    "          if sentence_score > 0:\n",
    "              if word_score > 0:\n",
    "                  boosted = word_score * 3\n",
    "                  if boosted < 0.1: \n",
    "                      boosted = 0.1\n",
    "                  adjusted_scores.append(boosted)\n",
    "              elif word_score < 0:\n",
    "                  more_neg = word_score * 1  \n",
    "                  adjusted_scores.append(more_neg)\n",
    "              else:\n",
    "                  adjusted_scores.append(0.0)\n",
    "          else:\n",
    "              if word_score < 0:\n",
    "                  more_neg = word_score * 3 \n",
    "                  if more_neg > -0.1: \n",
    "                      more_neg = -0.1\n",
    "                  adjusted_scores.append(more_neg)\n",
    "              elif word_score > 0:\n",
    "                  suppressed = word_score * 0.2\n",
    "                  adjusted_scores.append(suppressed)\n",
    "              else:\n",
    "                  adjusted_scores.append(0.0)\n",
    "                  \n",
    "      return np.array(adjusted_scores) if len(adjusted_scores) > 0 else np.array([0.0])\n",
    "\n",
    "\n",
    "  df_balanced = pd.read_csv(\"train_balanced.csv\")\n",
    "  df_balanced['adjusted_word_scores'] = df_balanced['cleaned_tweet'].apply(adjust_word_scores)\n",
    "\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "  X = csr_matrix(X)\n",
    "\n",
    "  X_fused = X.copy()\n",
    "  feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "  row_indices, col_indices = X.nonzero()\n",
    "\n",
    "  for row, col in zip(row_indices, col_indices):\n",
    "      word = feature_names[col]\n",
    "      words_in_sentence = df_balanced['cleaned_tweet'].iloc[row].lower().split()\n",
    "      if word in words_in_sentence:\n",
    "          word_idx = words_in_sentence.index(word)\n",
    "          try:\n",
    "              score = df_balanced['adjusted_word_scores'].iloc[row][word_idx]\n",
    "              X_fused[row, col] *= (1 + score)\n",
    "          except IndexError:\n",
    "              pass\n",
    "\n",
    "  y = df_balanced['class']\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "  model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  print(\"Train Accuracy:\", model.score(X_train, y_train))\n",
    "  print(\"Test Accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "  test_texts = [\"He is so fucking annoying\", \"I am so fucking happy for you\"]\n",
    "  df_test = pd.DataFrame({\"text\": test_texts})\n",
    "  df_test[\"adjusted_word_scores\"] = df_test[\"text\"].apply(adjust_word_scores)\n",
    "\n",
    "  X_test_vec = vectorizer.transform(df_test[\"text\"])\n",
    "  X_test_vec = csr_matrix(X_test_vec)\n",
    "  X_test_fused = X_test_vec.copy()\n",
    "\n",
    "  row_indices, col_indices = X_test_vec.nonzero()\n",
    "  for row, col in zip(row_indices, col_indices):\n",
    "      word = feature_names[col]\n",
    "      words_in_sentence = df_test[\"text\"].iloc[row].lower().split()\n",
    "      if word in words_in_sentence:\n",
    "          idx = words_in_sentence.index(word)\n",
    "          try:\n",
    "              score = df_test['adjusted_word_scores'].iloc[row][idx]\n",
    "              X_test_fused[row, col] *= (1 + score)\n",
    "          except IndexError:\n",
    "              pass\n",
    "\n",
    "  preds = model.predict(X_test_fused)\n",
    "  print(\"Predictions:\", preds)\n",
    "\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Guidelines [Still thinking on how to craft]\n",
    "\n",
    "- **Deployment:**  \n",
    "  - Steps to deploy the model locally or on a server.\n",
    "  \n",
    "- **Running Inference:**  \n",
    "  - How to use the model for predictions.\n",
    "  - Example:\n",
    "    ```python\n",
    "    from model import load_model, predict\n",
    "    model = load_model('model_checkpoint.pth')\n",
    "    prediction = predict(model, input_text=\"Sample text\")\n",
    "    print(prediction)\n",
    "    ```\n",
    "\n",
    "- **Integration:**  \n",
    "  - Instructions for integrating with other systems if applicable.\n",
    "\n",
    "- **Examples:**  \n",
    "  - Provide real-world usage examples with expected outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting & FAQ [Still thinking on how to craft]\n",
    "\n",
    "- **Common Issues:**  \n",
    "  List common problems and suggested solutions.\n",
    "\n",
    "- **Error Messages:**  \n",
    "  - Explanation of error messages and how to address them.\n",
    "\n",
    "- **FAQ:**  \n",
    "  - Provide answers to frequently asked questions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; \n",
    "              color:#1DA1F2; \n",
    "              font-size:150%;\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; \n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; \n",
    "              margin: auto;\">1 |\n",
    "  Importing necessary modules\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Natural Language Processing (NLP) Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display_html\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.tree import plot_tree\n",
    "import plotly.express as px\n",
    "\n",
    "# Data Manipulation and Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from pympler import asizeof\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; \n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; \n",
    "              border-radius:20px; \n",
    "              border-width: 3px; \n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto;\n",
    "              margin: auto;\">2 |\n",
    "  Exploratory Data Analysis\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the target variable has 3 classes (Hate speech, Offensive Language, Neither). The dataset is dominated by offensive lanugage tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better view the dataset, a labels column would be created and the classification of the tweet will be mapped according to the class digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {0: 'Hate Speech', 1: 'Offensive Language', 2: 'Neither'}\n",
    "class_counts = df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df['class'].map(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#1DA1F2', '#1476D6', '#0A4C8A']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "class_counts.rename(index=class_labels).plot(kind='bar', color=palette)\n",
    "plt.title('Class count of the tweets')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this countplot it is evident that majority of the tweets are classified as offensive language. Hate speech and neither class on the other hand has a narrower gap between them. This class imbalance is a critical factor to be considered during data cleaning & preprocessing to ensure that the model does not become biased towards the dominant class which could affect the overall performance and fairless of the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values=class_counts.values, names=class_labels, color=class_labels,\n",
    "             color_discrete_map={'Hate Speech':'cyan',\n",
    "                                 'Offensive Language':'royalblue',\n",
    "                                 'Neither':'darkblue'})\n",
    "\n",
    "fig.update_traces(textinfo='percent+label', texttemplate='%{label}<br>%{percent:.1%}', \n",
    "                  hovertemplate='%{label}: %{value}', \n",
    "                  marker=dict(line=dict(color='white', width=2)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title = \"Percentage distribution of the tweets by class\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pie chart we are able to see the percentage breakdown of the different tweet classes.\n",
    "| class       | percentage |\n",
    "|--------|-------|\n",
    "| Hate Speech  | 77.4%    |\n",
    "| Offensive Language  | 16.8%     |\n",
    "| Neither  | 5.8%     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_tweets = df[df['labels'] == 'Offensive Language']['tweet']\n",
    "\n",
    "text = ' '.join(offensive_tweets)\n",
    "plt.figure(figsize=(20,15), facecolor='None')\n",
    "wordcloud = WordCloud(max_words=500, width=1600, height=800, colormap='Blues').generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most common words in offensive tweets', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the word cloud we are able to see the common words present in the offensive tweets. \n",
    "Words such as 'bitch', 'RT', 'hoe', 'bitches', 'fuck' are most commonly present words.\n",
    "\n",
    "However, the term 'RT' refers to retweet and it is not meaningful word in the sentence. By removing the term 'RT' from the sentences the meaning of the setence would not change and the NLP model will be able to focus on the actual words determing the classification. Will handle this during the data cleansing portion further down the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tweets = df[df['labels'] == 'Hate Speech']['tweet']\n",
    "\n",
    "text = ' '.join(hate_tweets)\n",
    "plt.figure(figsize=(20,15), facecolor='None')\n",
    "wordcloud = WordCloud(max_words=500, width=1600, height=800, colormap='Blues').generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most common words in hate speech tweets', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the word cloud we are able to see the common words present in the hate speech tweets. \n",
    "Words such as 'faggot, 'nigga', 'RT', 'hoe', 'bitch', 'fuck' are most commonly present words.\n",
    "\n",
    "Similar effect of term 'RT' to the modelling in the future (As explained above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_tweets = df[df['labels'] == 'Neither']['tweet']\n",
    "\n",
    "text = ' '.join(normal_tweets)\n",
    "plt.figure(figsize=(20,15), facecolor='None')\n",
    "wordcloud = WordCloud(max_words=500, width=1600, height=800, colormap='Blues').generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most common words in normal tweets', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the word cloud we are able to see the common words present in the normal tweets. \n",
    "Words such as 'lol', 'RT', 'CO', 'bird', 'trash', 'yell', 't' are most commonly present words.\n",
    "\n",
    "Similar effect of term 'RT' to the modelling in the future (As explained above).\n",
    "Term's such as 't', 'co' also comes under this case.\n",
    "\n",
    "However, from the 3 word clouds we could not see the exact count of the common words occurences. Hence, we will generate a dataframe to view this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_speech = df[df['labels'] == 'Neither']\n",
    "hate_speech = df[df['labels'] == 'Hate Speech']\n",
    "offensive_speech = df[df['labels'] == 'Offensive Language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_speech.loc[:, 'word_list'] = normal_speech['tweet'].apply(lambda x: str(x).split())\n",
    "hate_speech.loc[:, 'word_list'] = hate_speech['tweet'].apply(lambda x: str(x).split())\n",
    "offensive_speech.loc[:, 'word_list'] = offensive_speech['tweet'].apply(lambda x: str(x).split())\n",
    "\n",
    "def top_words(df, label):\n",
    "    top = Counter([item for sublist in df['word_list'] for item in sublist])\n",
    "    temp = pd.DataFrame(top.most_common(10))\n",
    "    temp.columns = [label, 'count']\n",
    "    return temp.style.background_gradient(cmap='Blues')\n",
    "\n",
    "normal_words = top_words(normal_speech, 'Normal Words')\n",
    "hate_words = top_words(hate_speech, 'Hate Speech Words')\n",
    "offensive_words = top_words(offensive_speech, 'Offensive Words')\n",
    "\n",
    "html = f\"\"\"\n",
    "<div style='display: flex;  gap: 20px; text-align:center;'>\n",
    "    <div>{offensive_words._repr_html_()}</div>\n",
    "    <div>{hate_words._repr_html_()}</div>\n",
    "    <div>{normal_words._repr_html_()}</div>\n",
    "</div>\n",
    "\"\"\"\n",
    "display_html(html, raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this majority of the words are stop words. Stopwords are used commonly in english sentences for grammer and better flow.\n",
    "\n",
    "In English, examples of stop words include:\n",
    "\n",
    "Articles: a, an, the\n",
    "\n",
    "Conjunctions: and, but, or\n",
    "\n",
    "Prepositions: in, on, at, with\n",
    "\n",
    "Pronouns: he, she, it, they\n",
    "\n",
    "Common verbs: is, am, are, was, were, be, being, been\n",
    "\n",
    "Therefore, to get the exact count of the common words bases on the labels, we will need to omit the stop words. Even though this process is supposed to be done under data cleansing, for visualisation purposes we will remove the stop words now and generate the dataframes again (Would use the respective word classes created instead of the tweet class). Tweet class would be properly cleansed in the data cleaning/preprocessing section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = str(text).split() \n",
    "    clean = [word for word in words if word.lower() not in stop_words]  \n",
    "    return \" \".join(clean)  \n",
    "\n",
    "normal_speech['tweet'] = normal_speech['tweet'].apply(remove_stopwords)\n",
    "hate_speech['tweet'] = hate_speech['tweet'].apply(remove_stopwords)\n",
    "offensive_speech['tweet'] = offensive_speech['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_speech.loc[:, 'word_list'] = normal_speech['tweet'].apply(lambda x: str(x).split())\n",
    "hate_speech.loc[:, 'word_list'] = hate_speech['tweet'].apply(lambda x: str(x).split())\n",
    "offensive_speech.loc[:, 'word_list'] = offensive_speech['tweet'].apply(lambda x: str(x).split())\n",
    "\n",
    "normal_words = top_words(normal_speech, 'Normal Words')\n",
    "hate_words = top_words(hate_speech, 'Hate Speech Words')\n",
    "offensive_words = top_words(offensive_speech, 'Offensive Words')\n",
    "\n",
    "html = f\"\"\"\n",
    "<div style='display: flex;  gap: 20px; text-align:center;'>\n",
    "    <div>{offensive_words._repr_html_()}</div>\n",
    "    <div>{hate_words._repr_html_()}</div>\n",
    "    <div>{normal_words._repr_html_()}</div>\n",
    "</div>\n",
    "\"\"\"\n",
    "display_html(html, raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to see the number of occurences for each of the top 10 words for the respective classes. We can see that the word 'bitch' and 'bitches' dominate the offensive words category if we exclude 'RT'. However, for the hate speech words and normal words, they have a relatively balanced distribution. This might be due to the large amount of data for the offensive class causing such skewing to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_length'] = df['tweet'].apply(len)\n",
    "\n",
    "fig = px.box(df, \n",
    "             x='class', \n",
    "             y='tweet_length', \n",
    "             color='class', \n",
    "             title='Tweet Length Distribution by Class',\n",
    "             template='plotly_white', \n",
    "             color_discrete_sequence=['royalblue', 'cyan', 'blue'])\n",
    "\n",
    "fig.update_traces(marker=dict(opacity=0.8, \n",
    "                              line=dict(width=2, color='#0077ff')))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_font=dict(size=24, color='#0057d9', family='Courier New'),\n",
    "    xaxis_title=\"Class\",\n",
    "    yaxis_title=\"Tweet Length\",\n",
    "    paper_bgcolor='white',\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(color='#0057d9', family='Courier New'),\n",
    "    hoverlabel=dict(bgcolor='#e6f2ff', font=dict(color='#002bb8')),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(showgrid=True, gridwidth=0.8, gridcolor='#cce6ff')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=0.8, gridcolor='#cce6ff')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the box plot we can clearly see that the offensive language class tends to have longer tweets. This means that offensive tweets often come in long rants or heated replies. Whereas the hate speech and normal speech classes tends to have a fewer outliers with a more compact distribution in the tweet length.\n",
    "\n",
    "If we take a look at the median:\n",
    "Normal speech has the longest median of 97.\n",
    "Hate speech has a median of 82.\n",
    "Offensive speech has a median of 78 with many extreme outliers leading up to 758.\n",
    "\n",
    "We can conclude that the shorter the tweet, the more liekly it is to be offensive or hateful.\n",
    "However, offensive tweets have the most variation in length, meaning people express anger both in short insults or long heated arguments.\n",
    "\n",
    "Therefore, it is important to normalize the tweet length before feeding it into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; /* Smaller padding */\n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; /* Smaller Font */\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; /* Thinner Border */\n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; /* Smaller Width */\n",
    "              margin: auto;\">3 |\n",
    "  Data Preprocessing & Data Cleaning\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to classify whether a tweet is positive or negative we will merge the offensive language and hate speech to negative tweet and consider the neither label as a positive tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['class'].apply(lambda x: 0 if x in [0, 1] else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {0: 'negative', 1: 'positive'}\n",
    "df['labels'] = df['class'].map(class_labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now clean the data before starting on feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('not')  # Keeping 'not' as it is important in negation (Can change the meaning of the sentence)\n",
    "\n",
    "def clean(text):\n",
    "\n",
    "    # Remove 'RT' (Retweet indicator)\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "\n",
    "    # Remove URLs \n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove Twitter handles \n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "\n",
    "    # Removing the stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Rejoin the words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'cleaned_tweet'] = df['tweet'].apply(clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the outliers from the dataset based on the tweet length, so as to have a fair & balanced modelling in the later part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "\n",
    "tweet_length = df['cleaned_tweet'].apply(len)\n",
    "df['tweet_length'] = df['cleaned_tweet'].apply(len)\n",
    "\n",
    "Q1 = tweet_length.quantile(0.25)\n",
    "Q3 = tweet_length.quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df = df[(tweet_length >= lower_bound) & (tweet_length <= upper_bound)]\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this process we can see that 46 outlier data was removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['tweet', 'tweet_length'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the duplicate rows to prevent the biasness during modelling in the next phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; /* Smaller padding */\n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; /* Smaller Font */\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; /* Thinner Border */\n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; /* Smaller Width */\n",
    "              margin: auto;\">4 |\n",
    "  Feature Engineering\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p><strong>Stemming and Lemmatization</strong> are processes that reduce word variants to root form.</p>\n",
    "</div>\n",
    "<div style=\"display: flex; align-items: center; justify-content: space-between;\">\n",
    "  <div>\n",
    "    <p>Stemmers eliminate word suffixes by running input word tokens against a pre-defined list of common suffixes. In NLP use cases, especially in sentiment analysis, it is important to perform stemming as getting the base word is important to classify if a word is positive or negative.</p>\n",
    "    <p>The common stemmer algorithms are <strong>Snowball</strong> and <strong>Porter</strong> stemmers which uses mathematical methods to eliminate suffixes.</p>\n",
    "  </div>\n",
    "</div>\n",
    "<div style=\"display: flex; align-items: center; justify-content: space-between;\">\n",
    "  <div>\n",
    "    <p>The purpose of lemmatization is the same as stemming, however it overcomes the drawbacks of stemming. While stemming merely removes common suffixes from the end of the word, lemmatization ensures that the output word is an existing normalized form of the word that can be found in the english dictionary (in the context of english words).</p> \n",
    "    <p>Therefore, lemmatization gives a more meaningful output for NLP tasks such as sentiment analysis.</p>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"stemming.png\" alt=\"Image Description\" width=\"300\" style=\"border-radius: 20px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  <img src=\"lemmatization.png\" alt=\"Image Description\" width=\"300\" style=\"border-radius: 20px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  \n",
    "  <div style=\"margin-left: auto;\">\n",
    "    <p style=\"font-size: 18px; color: white;\">\n",
    "      Pros & Cons of Stemming and Lemmatization\n",
    "    </p>\n",
    "    <p>Stemming is considered as a fast approach. However, the trade off is that the output might have inacuuracies and may differ in the meaning of the text.</p>\n",
    "    <p>On the other hand lemmatization produces a better results by preserving the meaning and grammatical function. However, it is a much slwower process due to more complex linguistic analysis, including dictionary lookups and part-of-speech tagging, to accurately derive the base form of a word based on its context.</p>\n",
    "    <p>One good example of Stemming vs Lemmatizaton will be on the word 'Caring'. Stemming will result in 'Car' which is completely a different word and holds a different meaning. Whereas, lemmatization will result in 'Care' which is a more accurate and applicable data when it comes to sentiment analysis.</p>\n",
    "    <p>Therefore, in this project lemmatization will be utilised.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(lemmatize_text) \n",
    "\n",
    "df[['cleaned_tweet']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling & Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Oversampling and Undersampling</strong> are techniques used to balance datasets that have an uneven distribution of classes. .</p>\n",
    "\n",
    "Oversampling replicates samples from the minority class. This is to even out the distribution with the majority class. However, this creates artificial class distributions that are different from real-world conditions.\n",
    "\n",
    "Undersampling removes samples form the majoirty class. This is to even out the distribution with the minority class. However, undersampling could lose valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"sampling.png\" alt=\"Image Description\" width=\"700\" style=\"border-radius: 10px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  \n",
    "  <div style=\"margin-left: auto;\">\n",
    "    <p>From looking at the pros and cons of the two sampling techniques, we can conclude that undersampling is better as it does not create artificial data which could heavily impact NLP tasks such as sentiment analysis. Fortunately, in our case we will be using undersampling.</p>\n",
    "    <p>This is because, the exploratory data analysis we found out the the offensive class had the majority of the data. Moreover, during our data preprocessing steps we combined the offensive and hate speech classes and classified them as negative class. This means that the data imbalance got worse as now the ratio between negative and positive class is on a wider range.</p>\n",
    "    <p>Therefore, an undersampling approach has to be taken to overcome the class imbalance.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority=df[(df['class']==0)] # Negative\n",
    "df_minority=df[(df['class']==1)] # Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority_undersample=resample(df_majority,\n",
    "                                 replace=False,\n",
    "                                 n_samples=df_minority.shape[0],\n",
    "                                 random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_majority_undersample, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist = df_balanced['class'].value_counts().reset_index()\n",
    "class_dist.columns = ['Class', 'Percentage']\n",
    "class_dist['Percentage'] = class_dist['Percentage'].div(df.shape[0]).multiply(100).round(3)\n",
    "\n",
    "print(tabulate(class_dist, headers='keys'))\n",
    "print('------------------------------------')\n",
    "print(\"Balanced dataset's size: \" + str(df_balanced.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the undersampling process now both the classes have even distribution of data summing up to approximately 34% of the dataset (8076 entries). Even though we lost bulk of the dataset, the current dataset would be a much better fit for the modelling compared to the first iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and Sentiment Reasoner) is a pre-trained sentiment analysis model built specifically for social media. \n",
    "It allows intensity scores to be generated based the emotional value behind the text. \n",
    "\n",
    "VADER gives 4 sentiment scores:\n",
    "\n",
    "| Score       | What It Means       |        | Compound Score | Meaning     |\n",
    "|-------------|--------------------|--------|----------------|-------------|\n",
    "| pos        | Positive Percentage |        | > 0.05        | Positive    |\n",
    "| neu        | Neutral Percentage  |        | -0.05 to 0.05 | Neutral     |\n",
    "| neg        | Negative Percentage |        | < -0.05       | Negative    |\n",
    "| compound   | Final Sentiment Score |       |              |            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"emotions.webp\" alt=\"Image Description\" width=\"700\" style=\"border-radius: 10px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  \n",
    "  <div style=\"margin-left: auto;\">\n",
    "    <p>Applying VADER to generate a new column in the dataset causing the dataset to be more informative.</p>\n",
    "    <p>We are able to identify the hidden meaning behind the tweets. Not all negative tweets is reflective of negative emotions. There are hidden meaning such as positive sacarsm, positive negation. For instace phrases such as <strong>'not bad'</strong> is a positive sacarsm and without the compound score it can be misclassified by the model during the modelling phase of the project. </p>  \n",
    "    <p>Ultimately, the ML model do not understand emotions — it only works with numbers. Therefore, by adding these sentiment scores as extra numerical features, the model will have more context about what the text means emotionally resulting in better predictions.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "df_balanced['sentiment_score'] = df_balanced['cleaned_tweet'].apply(get_sentiment)\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Factor/Annotator Agreement score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"weights.png\" alt=\"Image Description\" width=\"500\" style=\"border-radius: 10px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  \n",
    "  <div style=\"margin-left: auto;\">\n",
    "    <p>Weight score refers to the annotator agreement score based on the count of how many humans classified the tweet as hate_speech, offensive, or neutral. \n",
    "By adding this feature, it gives the model more context on the human perception of the tweet. This feature will act as the confidence score for the class labels (target), allowing the model to weigh highly agreed classifications more heavily while treating uncertain classifications with lower priority during the training phase.</p>\n",
    "\n",
    "<p>To calulate this score we will use the <strong>Disagreement Penalty Method</strong>. The goal of this method is to assign higher weight to tweets where there is stronger agreement while penalizing tweets with high disagreement.</p>\n",
    "\n",
    "\n",
    "<strong>To sum it up<strong>:\n",
    "<table style=\"width:100%; border-collapse: collapse; text-align: left; margin-bottom: 20px;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"padding: 10px; border: 1px solid #ddd;\">Advantage</th>\n",
    "      <th style=\"padding: 10px; border: 1px solid #ddd;\">Explanation</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"padding: 10px; border: 1px solid #ddd;\"><b>Improves Model Performance</b></td>\n",
    "      <td style=\"padding: 10px; border: 1px solid #ddd;\">Helps the model prioritize highly agreed classifications, leading to better performance during the modelling phase.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 10px; border: 1px solid #ddd;\"><b>Handles Ambiguity</b></td>\n",
    "      <td style=\"padding: 10px; border: 1px solid #ddd;\">Asssits the model in identifying potentially confusing or subjective tweets</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 10px; border: 1px solid #ddd;\"><b>Enhances Data Quality</b></td>\n",
    "      <td style=\"padding: 10px; border: 1px solid #ddd;\">Provides an additional layer of information beyond just the tweets</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "def calculate_weight(row):\n",
    "    total =  row[['hate_speech_count', 'offensive_language_count', 'neither_count']].sum()\n",
    "    majority =  row[['hate_speech_count', 'offensive_language_count', 'neither_count']].max()\n",
    "    remaining = total - majority\n",
    "\n",
    "    return majority - alpha * remaining\n",
    "\n",
    "df_balanced['weight'] = df_balanced.apply(calculate_weight, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This marks the end of feature engineering! We will drop the columns that are not needed for the modelling and prepare the dataset for the modelling phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.drop(['count', 'hate_speech_count', 'offensive_language_count', 'neither_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced # Final dataset that will be used for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a final check if the dataset is clean and ready for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced[df_balanced['cleaned_tweet'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_balanced[df_balanced['cleaned_tweet'].str.strip() != '']\n",
    "df_balanced[df_balanced['cleaned_tweet'] == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a copy of the latest cleaned dataset to use in the future if ever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.to_csv('train_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; /* Smaller padding */\n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; /* Smaller Font */\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; /* Thinner Border */\n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; /* Smaller Width */\n",
    "              margin: auto;\">5 |\n",
    "  Modelling & Evaluation\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer vs Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer does the job of creating a word count table. It takes in the collection of text data and converts it into a matrix of token counts. Each row represents the text/sentence and each column represents a unique word (or token). The values indicates how many times each word appears within the sentence/text.\n",
    "\n",
    "Suppose you have three sentences:\n",
    "\n",
    "\"I love coding.\"\n",
    "\"Coding is fun.\"\n",
    "\"I love learning new things.\"\n",
    "Using CountVectorizer, the result might look something like this:\n",
    "\n",
    "|        | coding | fun | i | is | learning | love | new | things |\n",
    "|--------|-------|-----|---|----|----------|------|-----|-------|\n",
    "| Doc 1  | 1     | 0   | 1 | 0  | 0        | 1    | 0   | 0     |\n",
    "| Doc 2  | 1     | 1   | 0 | 1  | 0        | 0    | 0   | 0     |\n",
    "| Doc 3  | 0     | 0   | 1 | 0  | 1        | 1    | 1   | 1     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer is an extension of CountVectorizer. While CountVectorizer counts the words, TfidfVectorizer goes a step further and addtionally considers the importance of words across all the sentences. It assigns more weight to words that appear more frequently in a single input but are rare across other inputs, making it to better distinguish between words like \"a\" and actual meaningful terms.\n",
    "\n",
    "|        | coding | fun | i | is | learning | love | new | things |\n",
    "|--------|-------|-----|---|----|----------|------|-----|-------|\n",
    "| Doc 1  | 0.70710678     | 0   | 0.70710678 | 0  | 0        | 0.70710678   | 0   | 0     |\n",
    "| Doc 2  | 0.4736296     | 0.4736296   | 0 | 0.40204024  | 0       | 0    | 0   | 0     |\n",
    "| Doc 3  | 0     | 0   | 0.52863461 | 0  | 0.40204024        | 0.40204024    | 0.52863461   | 0.52863461     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streamlining Classification Algorithms with Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"pipeline.png\" alt=\"Image Description\" width=\"300\" style=\"border-radius: 10px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  \n",
    "  <div style=\"margin-left: auto;\">\n",
    "    <p>The pipeline class allows for a more streamlined and modular machine learning workflow by chaining multiple moedlling steps togeather. This simplies the code and avoids code duplication while also eliminating the risk of data leakage by making sure that transformations are applied consistently.</p>\n",
    "    <p>The pipeline will automatically apply the fit_transform() for the CountVectorizer() first, then pass the transformed data into the model (MultinomialNB()).</p>  \n",
    "    Example:\n",
    "    <br>\n",
    "    CountVectorizer → .fit_transform(X_train)\n",
    "    <br>\n",
    "    MultinomialNB → .fit(transformed_data, y_train)\n",
    "    <br>\n",
    "    <br>\n",
    "    .fit_transform() will build the vocabulary and convert text to numbers using the vectorizers.\n",
    "    <br>\n",
    "    .fit() will then allow the classification model to learn the class probabilities from the numbers before making the prediction.\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_nb_cv = Pipeline(steps=[\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "pipe_nb_tfidf = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "pipe_lr_cv = Pipeline(steps=[\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "pipe_lr_tfidf = Pipeline(steps=[\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "pipe_rf_cv = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "pipe_rf_tfidf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "pipe_svm_cv = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "pipe_svm_tfidf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svm', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [pipe_nb_cv, pipe_nb_tfidf, pipe_lr_cv, pipe_lr_tfidf, pipe_rf_cv, pipe_rf_tfidf, pipe_svm_cv, pipe_svm_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base line evaluation (Using raw data) w/o hyperparameter tuning\n",
    "df=pd.read_csv(\"train.csv\")\n",
    "df['class'] = df['class'].apply(lambda x: 0 if x in [0, 1] else 1)\n",
    "class_labels = {0: 'negative', 1: 'positive'}\n",
    "df['labels'] = df['class'].map(class_labels)\n",
    "\n",
    "x = df['tweet']\n",
    "y = df['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train = pd.concat([x_train, y_train], axis=1)\n",
    "df_majority = df_train[df_train['class'] == 0]\n",
    "df_minority = df_train[df_train['class'] == 1]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
    "df_balanced_baseline = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "x_train = df_balanced_baseline['tweet']\n",
    "y_train = df_balanced_baseline['class']\n",
    "\n",
    "results_baseline = []\n",
    "\n",
    "for pipe in pipelines:\n",
    "    model_name = list(pipe.named_steps.keys())[1]\n",
    "    vectorizer = list(pipe.named_steps.keys())[0]\n",
    "    pipe.fit(x_train, y_train)\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    results_baseline.append({\n",
    "        'Model': model_name,\n",
    "        'Vectorizer': vectorizer,\n",
    "        'Accuracy': acc\n",
    "    })\n",
    "\n",
    "df_baseline = pd.DataFrame(results_baseline)\n",
    "\n",
    "df_baseline = df_baseline.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based of the baseline modelling, naive bayes tends to perform the best with accuracy levels ranging from 92-93%. Possible reasoning for such high accuracy levels can be due to the quality of the dataset. Since this is a human annotated dataset and a huge team was assigned to work on this, the data quality of this dataset is already top-notch. Meaning the classifiers were able to classify the tweets well even without the data being cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitial evaluation (Using cleansed data) w/o hyperparameter tuning\n",
    "x=df_balanced['cleaned_tweet']\n",
    "y=df_balanced['class']\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "results_initial = []\n",
    "\n",
    "for pipe in pipelines:\n",
    "    model_name = list(pipe.named_steps.keys())[1]\n",
    "    vectorizer = list(pipe.named_steps.keys())[0]\n",
    "    pipe.fit(x_train, y_train)\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    results_initial.append({\n",
    "        'Model': model_name,\n",
    "        'Vectorizer': vectorizer,\n",
    "        'Accuracy': acc\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_initial)\n",
    "\n",
    "df_results = df_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial evaluation was done based on the dataset after cleaning. As expected the accuracy levels rose. The best performing model was the random forest models which had accuracy levels ranging between 94-95% which is an improvement from the initial 92-93%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if by adding weights there is a improvment to the accuracy levels\n",
    "x=df_balanced['cleaned_tweet']\n",
    "y=df_balanced['class']\n",
    "weights = df_balanced['weight']\n",
    "\n",
    "x_train, x_test, y_train, y_test, weights_train, weights_test = train_test_split(x, y, weights, test_size=0.2, random_state=42)\n",
    "results_weight = []\n",
    "\n",
    "for pipe in pipelines:\n",
    "    model_name = list(pipe.named_steps.keys())[1]\n",
    "    vectorizer = list(pipe.named_steps.keys())[0]\n",
    "    if 'sample_weight' in pipe.named_steps[model_name].fit.__code__.co_varnames:  # Check if model supports weights\n",
    "        pipe.fit(x_train, y_train, **{f\"{model_name}__sample_weight\": weights_train})\n",
    "    else:\n",
    "        pipe.fit(x_train, y_train)  # No weights if model doesn't support it \n",
    "    y_pred = pipe.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    results_weight.append({\n",
    "        'Model': model_name,\n",
    "        'Vectorizer': vectorizer,\n",
    "        'Accuracy': acc\n",
    "    })\n",
    "\n",
    "df_results_weight = pd.DataFrame(results_weight)\n",
    "\n",
    "df_results_weight = df_results_weight.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_results_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the weights to the random forest model it can be seen that  the random forest model under count vectorizer improved in the accuracy levels whereas the random forest model under the TF-IDF vectorizer deproved. Reasoning for this is because under count vectorizer initially its just based of the number of occurrences by adding weights, the data set could be more informative causing good prediction.\n",
    "\n",
    "However, TF-IDF already gives more weight to rare words, so by adding more weights it might cause overfitting causing the accuracy levels to drop. Therfore, we should only add weights for the random forest model under cout vectorizer if we are using that for evaluation.\n",
    "\n",
    "From these 3 experiments we can conclude that random forest and logistic regression models work the best for this project. Let's now work on improving the accuracy level using hyperparameter tuning using GridSearch.\n",
    "\n",
    "Moving on with the project we will focus on the top 3 models for this project. Random forest, Logistic regression & Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; /* Smaller padding */\n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; /* Smaller Font */\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; /* Thinner Border */\n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; /* Smaller Width */\n",
    "              margin: auto;\">6 |\n",
    "  Hyperparameter tuning & Testing\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameter Grid for Random Forest\n",
    "# param_grid_rf = {\n",
    "#     'rf__n_estimators': [50, 100, 200, 300],          \n",
    "#     'rf__max_depth': [None, 10, 20, 30],          \n",
    "#     'rf__min_samples_split': [2, 5, 10, 20],      \n",
    "#     'rf__min_samples_leaf': [1, 2, 3, 4, 5]         \n",
    "# }\n",
    "\n",
    "# # Create pipelines for Random Forest\n",
    "# pipelines_rf = [pipe_rf_cv, pipe_rf_tfidf]\n",
    "\n",
    "# # Results List\n",
    "# tuning_results_rf = []\n",
    "\n",
    "\n",
    "# # Random Forest Tuning\n",
    "# for pipe in pipelines_rf:\n",
    "#     vectorizer = list(pipe.named_steps.keys())[0]\n",
    "#     grid = GridSearchCV(pipe, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "#     model_name = list(pipe.named_steps.keys())[1]\n",
    "#     grid.fit(x_train, y_train)\n",
    "    \n",
    "#     tuning_results_rf.append({\n",
    "#         'Model': 'Random Forest',\n",
    "#         'Vectorizer': vectorizer,\n",
    "#         'Best Params': grid.best_params_,\n",
    "#         'Best Score': grid.best_score_\n",
    "#     })\n",
    "    \n",
    "\n",
    "# df_tuning_results_rf = pd.DataFrame(tuning_results_rf)\n",
    "\n",
    "# print(\"Best Hyperparameters for Each Model:\")\n",
    "# for index, row in df_tuning_results_rf.iterrows():\n",
    "#     print(f\"Model: {row['Model']}\")\n",
    "#     print(f\"Vectorizer: {row['Vectorizer']}\")\n",
    "#     print(f\"Best Parameters: {row['Best Params']}\")\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a parameter grid for Logistic Regression\n",
    "# # Note: Keys are adjusted to match pipeline step names\n",
    "# param_grid_lr = {\n",
    "#     'lr__C': [0.01, 0.1, 1, 10, 100],        # Range of regularization strengths\n",
    "#     'lr__penalty': ['l1', 'l2'],            # Types of regularization\n",
    "#     'lr__solver': ['liblinear', 'saga'],    # Solvers that can handle l1, l2\n",
    "#     'lr__max_iter': [15000],                 # Increase max iterations to ensure convergence\n",
    "#     'lr__class_weight': ['balanced', None]  # Handle class imbalance\n",
    "# }\n",
    "\n",
    "# # Create pipelines for Logistic Regression\n",
    "# pipelines_lr = [pipe_lr_cv, pipe_lr_tfidf]\n",
    "\n",
    "# # Container for storing results\n",
    "# tuning_results_lr = []\n",
    "\n",
    "\n",
    "# for pipe in pipelines_lr:\n",
    "#     vectorizer = list(pipe.named_steps.keys())[0]\n",
    "#     grid = GridSearchCV( pipe,  param_grid_lr,  cv=5,  scoring='accuracy',  n_jobs=-1)\n",
    "#     grid.fit(x_train, y_train)\n",
    "    \n",
    "#     tuning_results_lr.append({\n",
    "#         'Model': 'Logistic Regression',\n",
    "#         'Vectorizer': vectorizer,\n",
    "#         'Best Params': grid.best_params_,\n",
    "#         'Best Score': grid.best_score_,\n",
    "#     })\n",
    "\n",
    "# df_tuning_results_lr = pd.DataFrame(tuning_results_lr)\n",
    "\n",
    "# print(\"Best Hyperparameters for Logistic Regression Models:\")\n",
    "# for index, row in df_tuning_results_lr.iterrows():\n",
    "#     print(f\"Model: {row['Model']}\")\n",
    "#     print(f\"Vectorizer: {row['Vectorizer']}\")\n",
    "#     print(f\"Best Parameters: {row['Best Params']}\")\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameter Grid for SVM\n",
    "# param_grid_svm = {\n",
    "#     'svm__C': [0.1, 1, 10, 100],               # Regularization parameter\n",
    "#     'svm__kernel': ['linear', 'rbf', 'poly'],    # Kernel type to be used\n",
    "#     'svm__gamma': ['scale', 'auto'],             # Kernel coefficient for 'rbf' and 'poly'\n",
    "# }\n",
    "\n",
    "# # Create pipelines for Random Forest and SVM\n",
    "# pipelines_svm = [pipe_svm_cv, pipe_svm_tfidf]\n",
    "\n",
    "# # Results List\n",
    "# tuning_results_svm = []\n",
    "\n",
    "\n",
    "# # Random Forest Tuning\n",
    "# for pipe in pipelines_svm:\n",
    "#     vectorizer = list(pipe.named_steps.keys())[0]\n",
    "#     grid = GridSearchCV(pipe, param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "#     model_name = list(pipe.named_steps.keys())[1]\n",
    "#     grid.fit(x_train, y_train)\n",
    "    \n",
    "#     tuning_results_svm.append({\n",
    "#         'Model': 'SVM',\n",
    "#         'Vectorizer': vectorizer,\n",
    "#         'Best Params': grid.best_params_,\n",
    "#         'Best Score': grid.best_score_\n",
    "#     })\n",
    "    \n",
    "\n",
    "# df_tuning_results_svm = pd.DataFrame(tuning_results_svm)\n",
    "\n",
    "# print(\"Best Hyperparameters for Each Model:\")\n",
    "# for index, row in df_tuning_results_svm.iterrows():\n",
    "#     print(f\"Model: {row['Model']}\")\n",
    "#     print(f\"Vectorizer: {row['Vectorizer']}\")\n",
    "#     print(f\"Best Parameters: {row['Best Params']}\")\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the model to test out on sample texts let's make sure the model does not overfit after using the best params from the hyperparameter tuning.\n",
    "\n",
    "As for the selection between CV & TF-IDF, TF-IDF performed better interms of accuracy levels. Hence, TF-IDF Vectorization was chosen as the final vectorizer due to its ability to put higher importance to rare words — a critical feature in sentiment classification tasks where slang, insults, or rare expressions often carry stronger sentiment.\n",
    "\n",
    "We will also caluclate the memory usage and time taken for the three models to complete their tasks.\n",
    "\n",
    "Shallow Memory Usage:\n",
    "This measures the memory taken up by the top-level object itself, without accounting for the memory of any objects it refers to. For example, using Python's sys.getsizeof(model) will tell you the size of the model object, but if the model contains nested objects (like arrays, lists, or dictionaries), their sizes won't be included. This approach might underestimate the total memory footprint.\n",
    "\n",
    "Deep Memory Usage:\n",
    "This approach recursively calculates the memory usage of the object and all objects it references. Tools like pympler.asizeof can be used to get this comprehensive measure. Deep memory usage is more representative of the total resources consumed by the model, as it includes every component that the model depends on.\n",
    "\n",
    "Furthermore, aside from the accuracy levels we would print out the misclassified tweets when the model is evaluated against the test set. This way we are able to see if there's any patterns observed on the three different models on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "y = df_balanced['class']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('RF.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('RF.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in Random Forest\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "y = df_balanced['class']\n",
    "\n",
    "model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "\n",
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('RF-Tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('RF-Tuned.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in Random Forest\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "y = df_balanced['class']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('LR.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('LR.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in Random Forest\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "y = df_balanced['class']\n",
    "\n",
    "model = LogisticRegression(C = 1, class_weight= 'balanced', max_iter= 11000, penalty= 'l1', solver= 'liblinear') \n",
    "\n",
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('LR-Tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('LR-Tuned.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in LR\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "y = df_balanced['class']\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('SVM.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('SVM.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in SVM\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "y = df_balanced['class']\n",
    "\n",
    "model = SVC(C=1, gamma='scale', kernel='linear')\n",
    "\n",
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('SVM-Tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('SVM-Tuned.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in SVM\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "As expected after the hyperparamter tuning, all the accuracy levels of the models rised.\n",
    "\n",
    "When we look at the number of misclassified tweets and the accuracy levels, random forest and logistic regression clearly outperforms support vector machines while random forest edging over logistic regression model by a very little percentage. Furthermore, the number of misclassifications between random forest and logistic regression was relatively similar with logistic regression missclassifying slightly more than random forest. \n",
    "\n",
    "While Logistic Regression was a useful baseline, its performance lagged behind Random Forest. Hence, moving from here we will focus on Random Forest (the strongest ensemble approach) versus SVM (a margin-based approach), as these two offer more competitive results and different modeling philosophies.\n",
    "\n",
    "However, at the final stage of the project we will test out the logistic regression model with the same tweaking done to the other two models to see if it manages to outperform the best model.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we can see that for both the models the training score and the testing score was above 90% which is close to the training score indicating that chances for overfitting is very little. However, it is clear that the random forest model performs slightly better than support vector machines on the testing set.\n",
    "\n",
    "Although SVM takes less memory space and is faster in training time,\n",
    "Random Forests generally have an edge over SVM (SVM with a non-linear kernel) when it comes to handling non-linear patterns and noisy data. \n",
    "\n",
    "Here’s why...\n",
    "\n",
    "Non-linear Relationships:\n",
    "Random Forests, as an ensemble of decision trees, naturally capture non-linear interactions in the data without requiring explicit transformation or kernel tricks. Each tree makes splits based on different subsets of features, allowing the ensemble to model complex relationships effectively.\n",
    "\n",
    "Robustness to Noise:\n",
    "The averaging of predictions from multiple trees in a Random Forest helps smooth out the impact of noisy or outlier data. This ensemble approach tends to be more robust compared to a single SVM model, which might be more sensitive to noise, especially if the kernel or hyperparameters are not tuned carefully.\n",
    "\n",
    "Model Flexibility:\n",
    "While SVM with a non-linear kernel (like RBF) can model complex boundaries, it requires careful parameter tuning (such as setting the right gamma and C values) to avoid overfitting to noise. Random Forests, on the other hand, often perform reasonably well with less intensive tuning due to their inherent ensemble nature.\n",
    "\n",
    "In summary, if our primary concern is dealing with non-linearity and noisy data, a Random Forest can often classify such cases more robustly than an SVM, especially when the data characteristics are challenging and require a model that adapts to a wide variety of patterns without extensive hyperparameter adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Moving into the revaluation we will work on improving the accuracy levels and reduce the number of misclassified tweets for the two differnet models. We will start with random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take a look at an example of how random forest models work in this case to better understand its flow so as to ease the process of debugging if we face any issues along the way.\n",
    "\n",
    "Example output from the vectorizer during training.\n",
    "\n",
    "|Word|   I    | hate | hello | sucks | yay | fuck | Label\n",
    "|------|------|------|-------|-------|------|------|------|\n",
    "|Tweet 1| 0.2  | 0.8  | 0.3   | 0     | 0    | 0    |positive|\n",
    "|Tweet 2| 0.2  | 0    | 0.3   | 0.9   | 0    | 0    |negative|\n",
    "|Tweet 3| 0.2  | 0.5    | 0     | 0.7    | 0    | 0.8    |positive|\n",
    "|Tweet 4| 0.2  | 0    | 0     | 0     | 0.1  | 0.9  |negative|\n",
    "\n",
    "Initially, the random forest model will randomly select rows (tweets) from the training data with replacement. This process is called bootstrapping. The number of trees is decided by the hyperparameter n_estimators.\n",
    "\n",
    "|Original Dataset|\tBootstrapped Sample (Tree 1)|\tBootstrapped Sample (Tree 2)|\tBootstrapped Sample (Tree 3)|\n",
    "|------|------|------|-------|\n",
    "|Tweet 1 (Positive)|\tTweet 1|\tTweet 3|\tTweet 2|\n",
    "|Tweet 2 (Negative)|Tweet 3\t|Tweet 4|\tTweet 1|\n",
    "|Tweet 3 (Positive)\t|Tweet 4|\tTweet 1|\tTweet 4|\n",
    "|Tweet 4 (Negative)\t|Tweet 2|\tTweet 3\t|Tweet 3|\n",
    "\n",
    "\n",
    "Following this, the random forest model will randomly select features (words) from the vectorized matrix.\n",
    "\n",
    "|Tree\t|Random Features Picked|\n",
    "|------|------|\n",
    "Tree 1|\thate, sucks, fuck|\n",
    "Tree 2|\thello, fuck, I|\n",
    "Tree 3|\thate, yay, hello|\n",
    "\n",
    "Now, the model will start splitting the tweets.\n",
    "\n",
    "Let's take tree 1 as an example.\n",
    "\n",
    "|Feature\t|Possible Thresholds|\n",
    "|------|------|\n",
    "|hate\t|0.8, 0.5, 0|\n",
    "|sucks|\t0.9, 0.7|\n",
    "|fuck|0.9|\n",
    "\n",
    "Now the random forest model will calculate the Gini for every possible split. \n",
    "\n",
    "This is a very tedious process that goes behind the scenes that need to be visualized to understand how the model works. We will assume after all the calculations, the gini index for 'hate' > 0.5 had a gini score of 0 (purest form). Hence the tree will use that as the first split. After the first split the same feature wont be used. Meaning now 'sucks' and 'fuck' will be used to continue splitting until they reach a gini score of 0 (purest form). Once all the leaf nodes (purest form) is attained the tree is completed.\n",
    "\n",
    "Example:\n",
    "\n",
    "|Tweet\t|hate Value|\tLabel|\tSplit|\n",
    "|------|------|------|-------|\n",
    "|Tweet 1|\t0.8|\tPositive|\tGroup 1| \n",
    "|Tweet 3|\t0.5|Positive|\tGroup 2 |\n",
    "|Tweet 2|\t0|\tNegative|\tGroup 2 |\n",
    "|Tweet 4|\t0|\tNegative|\tGroup 2 |\n",
    "\n",
    "The goal of the random forest model is to continue splitting until each node becomes pure. An example tree would look like this.\n",
    "\n",
    "\n",
    "                  hate > 0.5?\n",
    "                  /       \\\n",
    "           Positive      sucks > 0.7?\n",
    "                         /         \\\n",
    "                  Negative     fuck > 0.8?     \n",
    "                                  /       \\\n",
    "                             Negative    Positive \n",
    "\n",
    "\n",
    "This is just 1 tree, by default the random forest classifier would create 100 trees. The final classification of a certain input is then decided by the majority interms of classifications of the 100 trees.\n",
    "\n",
    "\n",
    "Now that we understand how a random forest classification model functions, we will make some decisions before moving onto the re-evaluation.\n",
    "\n",
    "We will be using a tuned random forest model with the cleansed data under the TF-IDF vectorizer without adding in the additional weights for our final re-evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; /* Smaller padding */\n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; /* Smaller Font */\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; /* Thinner Border */\n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; /* Smaller Width */\n",
    "              margin: auto;\">7 |\n",
    "  Re-evaluation & Further enhancements\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test out some sample tweets and see how well the model classifies them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "y = df_balanced['class']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the RandomForest model\n",
    "model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "texts = [\"He is so fucking annoying\", \"I am so happy for you\"]\n",
    "texts_vec = vectorizer.transform(texts)\n",
    "\n",
    "predictions = model.predict(texts_vec)\n",
    "\n",
    "for text, pred in zip(texts, predictions):\n",
    "    sentiment = 'Positive' if pred == 1 else 'Negative'\n",
    "    print(f\"{text} ➡️ {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By testing out the random forest model under TF-IDF vectorizer, the expected output was 2 different classes. But the model classified both as a positive prediction. \n",
    "\n",
    "Here's a possible reasoning for such classification:\n",
    "The key word in the first sentence is 'fucking' which we would expect to result in a negative emotion as the prediction.\n",
    "However, the word 'fucking' is not directly abussive in all cases. It can be used in both positive and negative phrases such as:\n",
    "\n",
    "\"That's fucking awesome\" (Positive)\n",
    "\"I'm so fucking happy\" (Positive)\n",
    "\"He's so fucking useless\" (Negative)\n",
    "\n",
    "So what might be happening is that since the vectorizers treat each word independently without context, 'fucking' is labelled to be positive possibly due to the higher weights from the positive samples. Even though 'useless' is negative, positive wins due to the higher weight on it (based of the tfidf which checks how rare the word is). Since 'fucking' has a higher weight due to it being a more common word in the data set compared to useless and that 'fucking' is associated with more positive tweets than negative tweets, the overall classification will then be considered as positive.\n",
    "\n",
    "To check if our theroy was correct, we checked it against a word which is usually associated with a negative meaning behing it. By testing our various phrases with the word 'bitch', as expected the results were always negative as the word has a high weight assigned to it as well as it being associated with more negative tweets than positive tweets.\n",
    "\n",
    "Ultimately, the case of \"He's so fucking useless\" is still classified wrongly. Therefore, the model has to be forced to look at phrases (context) instead of individual words. To tackle this the model we can use n-grams when vectorizing - a technique where words are grouped into consecutive pairs or triplets.\n",
    "\n",
    "N-grams are groups of consecutive words from a sentence. This will allow the model to understand the context instead of treating each word independently. By combining the words to form phrases and referencing back to the class for the tweet, there is surely a way lesser chance of classifying the tweet wrongly.\n",
    "\n",
    "| Word     | Prediction |\n",
    "|----------|-----------------------|\n",
    "| He       | Positive              |\n",
    "| is       | Positive              |\n",
    "| so       | Positive              |\n",
    "| fucking | Positive    |\n",
    "| useless  | Negative             |\n",
    "\n",
    "Final prediction: Positive\n",
    "\n",
    "| N-Gram Type | Example               | What the Model Sees | Prediction   |\n",
    "|-------------|-----------------------|--------------------|-------------|\n",
    "| Unigram     | \"fucking\" + \"useless\" | Independent Words  | Positive |\n",
    "| Bigram      | \"fucking useless\"     | Combined Phrase    | Negative |\n",
    "| Trigram     | \"so fucking useless\"   | Full Context      | Negative |\n",
    "\n",
    "Expected prediction: Negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying n-grams to the vectorizer the results remained the same. This means there is another unseen problem that has to be tackled.\n",
    "\n",
    "Possible reasoning behind this is due to the limited data on such specific phrases in the data set which can cause the prediction to then rely on the unigram which as explained earlier will result in a positive class.\n",
    "\n",
    "Therefore, there has to be another approach taken to tackle this issue. \n",
    "\n",
    "Let's make use of the sentiment_score derived using VADER. Instead of training the model just based on the cleansed data and the vectorizer, let's add in the element of sentiment_score to tackle this issue. By combining n-grams textual features with sentiment intensity scores, the model will be able to capture both lexical patterns and emotional context. This hybrid approach enhances performance on tweets which are harder to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Sentiment Score between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "df_balanced['sentiment_score'] = scaler.fit_transform(df_balanced[['sentiment_score']])\n",
    "\n",
    "# Fuse Sentiment into TF-IDF Scores\n",
    "X_fused = X.multiply(df_balanced['sentiment_score'].values.reshape(-1, 1))\n",
    "\n",
    "# Training the Model\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "texts = [\"He is so fucking annoying\", \"I am happy for you\"]\n",
    "df_test = pd.DataFrame({'test_tweet': texts})\n",
    "df_test['sentiment_score'] = df_test['test_tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "X_test_vec = vectorizer.transform(df_test['test_tweet'])\n",
    "df_test['sentiment_score'] = scaler.transform(df_test[['sentiment_score']])\n",
    "\n",
    "X_test_fused = X_test_vec.multiply(df_test['sentiment_score'].values.reshape(-1, 1))\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_fused)\n",
    "\n",
    "# Display Predictions\n",
    "for text, pred in zip(df_test['test_tweet'], predictions):\n",
    "    sentiment = 'Positive' if pred == 1 else 'Negative'\n",
    "    print(f\"{text} ➡️ {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we wanted to add the sentiment score as an additional feature. However, that will be cheating the random forest model. This is because the sentiment score is a guaranteed correct classification. Meaning in the random forest trees the nodes that has the feature as sentiment score will now check for the sentiment score of the input. Since, sentiment score has a clear distinction between positive and negative, the tree will be able to easily classify the text. Meaning this does not use any natural language processing techniques to classify the text. However, sentiment score can be used in other ways to value add in the training and testing of the model.\n",
    "\n",
    "To integrate sentiment knowledge without introducing additional features, we decided to fuse the sentiment scores into the TF-IDF vectorizer matrix through element-wise multiplication. This approach preserves the original dimensionality of the matrix while value adding to the feature with the emotional context. Once ammending the code and testing this approach, another problem occured which was that now the negative tweet was being classified properly but the positive tweet was being classified incorrectly. Hence, more debugging to come.\n",
    "\n",
    "After critcially analyzing the code and thinking through we realised that the fusion must also be applied during testing to ensure that the learned patterns are preserved. Without the sentiment information, the model lacks the context required to classify the tweets. If we only multiply the training set with the sentiment values and leave the testing set as plain TF-IDF, it's like training a different model than testing. Even though at first glance, we thought that it is cheating when we apply sentiment scores to the testing set, deep down it is a necessary machine learning process to be done. Whatever that is done to the training set has to be applied to the testing set. This won't be considered as cheating as we are not giving giving the testing set the actual classification labels. We are merely providing it the information to match the training set to obey the laws of machine learning. After ammending the code, we tested it out again. However, results were still not as expected showing negative & negative. Hence, there has to be something within the code that is causing this unexpected behaviour.\n",
    "\n",
    "After looking through the code, we wanted to check if it is due to the way the vectorizer was handling the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_matrix = pd.DataFrame(X_test_vec.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the df_test_matrix we can see that there are 99776 different features after fitting the vectorizer with the training set. This is the different terms that are under the vocubulary after using the vectorizer under n_grams(1,3). We will check if the words from the input tweets are present inside the vectorizer and has some value associated with it. From our understanding, the key words shouldn't be present in the features which is causing the wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test_matrix.loc[:, (df_test_matrix != 0).any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, key words such as 'fucking' is not present in the features. Meaning the model is not recognizing such word at all causing it it be dependent on other words which were present in the tweet such as he or happy. Meaning for the first tweet it was dependent on 'he' and 'so'. Chances are these words were associated with more negative tweets causing it be considered as a negative tweet. However, for the second tweet the only word which was present is 'happy' and it's clear that happy should have been associated with positive instead of negative. \n",
    "\n",
    "Initially the model was able to classify the second tweet correctly. However, now it is classifying it wrongly. The major difference was fusing in the sentiment score. To check what's happening lets check out the polarity score for this particular tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('I am happy for you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sentiment score for this tweet was 0.5719 and the vectorizer score for 'happy' was 1. By just multiplying it, it is going to result in a low score of 0.5719. This value might have fallen below the learned decision threshold in the random forest model causing the classification to be negative. In our case it was not extremely low however for other terms this value could drop down drastically. By directly multiplying the sentiment scores with the vectorized score, positive terms would be down-weighted based on the sentiment polarity scores. Therefore to counter this, we will add a 1 in. By adding a 1 in we can make sure that the vectorized weight is preserved while boosting and supressing correctly based on the sentiment score.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Sentiment | Scaled Value | Offset Fusion Multiplier | Effect     |\n",
    "|-----------|-------------|--------------------------|-----------|\n",
    "| +0.8      | +0.8        | 1 + 0.8 = **1.8x**      | BOOST   |\n",
    "| -0.8      | -0.8        | 1 + (-0.8) = **0.2x**   | SUPPRESS |\n",
    "| 0.0       | 0.0         | 1 + 0 = **1x**          | No Change  |\n",
    "\n",
    "As seen instead of the usual fusion multiplier, the offset fusion multiplier acheives the intended outcome. However, it is important to note that the sentiment must keep the negative values to make sure this works. Therefore, instead of using the minmax scaler, we will use the standard scaler now to scale the values between -1 to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Sentiment Score between -1 and 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_balanced = pd.read_csv('train_balanced.csv')\n",
    "df_balanced['sentiment_score'] = scaler.fit_transform(df_balanced[['sentiment_score']])\n",
    "\n",
    "vectorizer_second = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer_second.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "# Fuse Sentiment using the offset method\n",
    "X_fused = X.multiply(1 + df_balanced['sentiment_score'].values.reshape(-1, 1))\n",
    "\n",
    "# Training the Model\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "texts = [\"He is so fucking annoying\", \"I am happy for you\"]\n",
    "df_test = pd.DataFrame({'test_tweet': texts})\n",
    "df_test['sentiment_score'] = df_test['test_tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "X_test_vec = vectorizer_second.transform(df_test['test_tweet'])\n",
    "df_test['sentiment_score'] = scaler.transform(df_test[['sentiment_score']])\n",
    "\n",
    "X_test_fused = X_test_vec.multiply(1 + df_test['sentiment_score'].values.reshape(-1, 1))\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_fused)\n",
    "\n",
    "# Display Predictions\n",
    "for text, pred in zip(df_test['test_tweet'], predictions):\n",
    "    sentiment = 'Positive' if pred == 1 else 'Negative'\n",
    "    print(f\"{text} ➡️ {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected now the Tweet 'I am happy for you' is now correctly classified as a positive tweet. Now let's tackle the first tweet. As discussed earlier, the word 'fucking' was not present inside the vocabulary. Meaning the model was relying on the words, 'He' and 'so'. We thought that these two words contributed to negative earlier as these words were associated with negative tweets. However, after applying the offset fusion multiplier, we are able to see that these words actually was associated with positive tweets, causing the prediction to result in a positive classification. Even though we are back to square 1 in the prediction, we are sure the model has evolved for the better. Let's now overcome this issue. The problem now is that, the word 'fucking' is not in the vocabulary and does not have any weights associated with it.\n",
    "\n",
    "This can be because of lemmatization done during feature engineering. When the data was lemmatized, words such as 'fucking' would have been lemmatized to 'fuck' which explains why the word, 'fucking' was not present in the vocabulary. We will confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_text('fucking') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can indeed confirm that the word 'fucking' was lemmatized to 'fuck'. Therefore, instead of translating the words to new words, we will now add in the new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, get_wordnet_pos(token))\n",
    "        lemmatized_tokens.append(token)  # Add Original Word\n",
    "        if lemma != token:\n",
    "            lemmatized_tokens.append(lemma)  # Add Lemmatized Word\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "final_df = pd.read_csv('cleaned_dataset.csv')\n",
    "final_df = final_df.dropna()\n",
    "final_df['cleaned_tweet'] = final_df['cleaned_tweet'].apply(lemmatize_text) \n",
    "final_df[['cleaned_tweet']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will add all the other feature engineering steps into one cell to maintain the same flow. Will be ommiting the weights factor step as we will not be using them in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority=final_df[(final_df['class']==0)]\n",
    "df_minority=final_df[(final_df['class']==1)] \n",
    "\n",
    "df_majority_undersample=resample(df_majority,\n",
    "                                 replace=False,\n",
    "                                 n_samples=df_minority.shape[0],\n",
    "                                 random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_majority_undersample, df_minority])\n",
    "df_balanced['sentiment_score'] = df_balanced['cleaned_tweet'].apply(get_sentiment)\n",
    "df_balanced.drop(['count', 'hate_speech_count', 'offensive_language_count', 'neither_count'], axis=1, inplace=True)\n",
    "df_balanced = df_balanced[df_balanced['cleaned_tweet'].str.strip() != '']\n",
    "df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will make a copy of the latest cleaned dataset with the addition of the existing words together with the lemmatized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.to_csv('train_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now we can get back to running our evaluation of the model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Sentiment Score between -1 and 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_balanced = pd.read_csv('train_balanced.csv')\n",
    "df_balanced['sentiment_score'] = scaler.fit_transform(df_balanced[['sentiment_score']])\n",
    "\n",
    "vectorizer_final = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer_final.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "# Fuse Sentiment using the offset method\n",
    "X_fused = X.multiply(1 + df_balanced['sentiment_score'].values.reshape(-1, 1))\n",
    "\n",
    "# Training the Model\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "texts = [\"He is so fucking annoying\", \"I am happy for you\"]\n",
    "df_test = pd.DataFrame({'test_tweet': texts})\n",
    "df_test['sentiment_score'] = df_test['test_tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "X_test_vec = vectorizer_final.transform(df_test['test_tweet'])\n",
    "df_test['sentiment_score'] = scaler.transform(df_test[['sentiment_score']])\n",
    "\n",
    "X_test_fused = X_test_vec.multiply(1 + df_test['sentiment_score'].values.reshape(-1, 1))\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_fused)\n",
    "\n",
    "# Display Predictions\n",
    "for text, pred in zip(df_test['test_tweet'], predictions):\n",
    "    sentiment = 'Positive' if pred == 1 else 'Negative'\n",
    "    print(f\"{text} ➡️ {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still predicting the first tweet incorrectly, even after adding both the actual text and lemmatized text. Meaning, there's something else that is still causing the invalid prediction. We will print out the random trees to check what is actually going behind the scenes. This way we will be able find what might be possibly causing the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display scrollable output\n",
    "def make_scrollable(output, height=300):\n",
    "    scrollable_output = f\"\"\"\n",
    "    <div style=\"height: {height}px; overflow-y: scroll; border: 1px solid #ddd; padding: 10px;\">\n",
    "        {output}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(scrollable_output))\n",
    "\n",
    "# Function to make plots scrollable\n",
    "def make_scrollable_plots(plot_images, height=400):\n",
    "    plot_html = \"\"\n",
    "    for plot_img in plot_images:\n",
    "        plot_html += f'<img src=\"{plot_img}\" style=\"max-width: 100%; height: auto; margin-bottom: 10px;\">'\n",
    "    scrollable_plots = f\"\"\"\n",
    "    <div style=\"height: {height}px; overflow-y: scroll; border: 1px solid #ddd; padding: 10px;\">\n",
    "        {plot_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(scrollable_plots))\n",
    "\n",
    "used_trees = []\n",
    "final_decisions = []\n",
    "comparison_data = []\n",
    "plot_images = []\n",
    "output_folder = \"trees\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Convert Sparse Matrix to CSR (for indexing support)\n",
    "X_test_fused = X_test_fused.tocsr()\n",
    "\n",
    "# Choose your sample tweet index\n",
    "sample_idx = 0\n",
    "X_sample = X_test_fused[sample_idx]\n",
    "\n",
    "# Feature Names\n",
    "feature_names = vectorizer_final.get_feature_names_out().tolist()\n",
    "\n",
    "# Loop through all the trees\n",
    "for i in range(100):\n",
    "    node_indicator = model.estimators_[i].decision_path(X_sample)\n",
    "    feature = model.estimators_[i].tree_.feature\n",
    "    leaf_id = model.estimators_[i].apply(X_sample)[0]\n",
    "\n",
    "    if len(node_indicator.indices) > 1:\n",
    "        used_trees.append(i)\n",
    "        last_decision = None\n",
    "\n",
    "        for node_id in node_indicator.indices:\n",
    "            if feature[node_id] != -2:  # Not leaf node\n",
    "                feature_value = X_sample[0, feature[node_id]]\n",
    "                threshold = model.estimators_[i].tree_.threshold[node_id]\n",
    "                operator = \"<=\" if feature_value <= threshold else \">\"\n",
    "                decision = \"LEFT ✅\" if feature_value <= threshold else \"RIGHT ❌\"\n",
    "\n",
    "                last_decision = {\n",
    "                    \"Node\": node_id,\n",
    "                    \"Feature\": feature_names[feature[node_id]],\n",
    "                    \"Comparison\": f\"{feature_value:.4f} {operator} {threshold:.4f}\",\n",
    "                    \"Threshold\": threshold,\n",
    "                    \"Decision\": decision\n",
    "                }\n",
    "                comparison_data.append(last_decision)\n",
    "\n",
    "        # Final Prediction\n",
    "        leaf_class = model.estimators_[i].tree_.value[leaf_id].argmax()\n",
    "        decision = \"Positive\" if leaf_class == 1 else \"Negative\"\n",
    "        final_decisions.append(decision)\n",
    "\n",
    "        if last_decision:\n",
    "            print(f\"Tree {i + 1} -> Final Decision: {decision} at Node {last_decision['Node']} \"\n",
    "                  f\"({last_decision['Feature']} {last_decision['Comparison']})\")\n",
    "\n",
    "        # Visualize Tree and save as images\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plot_tree(model.estimators_[i],\n",
    "                  feature_names=feature_names,\n",
    "                  class_names=[\"Negative\", \"Positive\"],\n",
    "                  filled=True,\n",
    "                  rounded=True,\n",
    "                  max_depth=4)\n",
    "        plt.title(f\"Tree {i + 1} (Final Decision = {decision})\")\n",
    "\n",
    "        # Save plot to image\n",
    "        plot_image_path = os.path.join(output_folder, f\"tree_{i+1}_plot.png\")\n",
    "        plt.savefig(plot_image_path)\n",
    "        plt.close()  # Close the plot after saving it\n",
    "        plot_images.append(plot_image_path)\n",
    "\n",
    "# Display all the plots in a scrollable container\n",
    "make_scrollable_plots(plot_images)\n",
    "\n",
    "# Pie Chart Voting Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie([final_decisions.count(\"Positive\"), final_decisions.count(\"Negative\")],\n",
    "        labels=[\"Positive\", \"Negative\"],\n",
    "        autopct='%1.1f%%',\n",
    "        colors=[\"#2196F3\", \"#3F51B5\"],\n",
    "        explode=(0.1, 0),\n",
    "        shadow=True)\n",
    "plt.title(\"Random Forest Voting Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the random trees generated we can see that majority of the trees that resulted in a positive predicition was due to the words associated with the final decision node before reaching the leaf node. Since the test set is using the same vectorizer that the training set was fitted with, the vocabulary is extensive. Due to this, the words which were not present in the testing phrases will be assigned a TF-IDF score of 0.00. As we can see from the chart 61% of the trees predicited the tweet as positive. These decisions was based of the words with the score of 0.00 which would not be a fair representation of the prediction. However, we will not be able to remove those words as the training set and testing set has to have the same number of features even if some of the features have values of 0.00. So need to find a way to handle this differently. One possible way is to increase the importance on the key negative word in hopes of more decisions being dependent on that word. Initially, when we derived the offset method to handle the fusing, the negative sentiment scores were being supressed. Now let's boost it as well to maintain the same effect the positive words get. This can be done by adding a abs to the sentiment scores. Futhermore, we will also reduce the ngrams back to 1 so as to place importance on singular words and keep the model more streamlined instead of overcomplicating it.\n",
    "\n",
    "We will now test out with the new changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.read_csv('train_balanced.csv')\n",
    "\n",
    "vectorizer_finally = TfidfVectorizer()\n",
    "X = vectorizer_finally.fit_transform(df_balanced['cleaned_tweet'])\n",
    "\n",
    "# Scale Sentiment Score between -1 and 1\n",
    "scaler = StandardScaler()\n",
    "df_balanced['sentiment_score'] = scaler.fit_transform(df_balanced[['sentiment_score']])\n",
    "\n",
    "# Fuse Sentiment using the offset method\n",
    "X_fused = X.multiply(1 + abs(df_balanced['sentiment_score'].values.reshape(-1, 1)))\n",
    "\n",
    "# Training the Model\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"He is so fucking annoying\", \"I am so happy for you\"]\n",
    "df_test = pd.DataFrame({'test_tweet': texts})\n",
    "df_test['sentiment_score'] = df_test['test_tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "X_test_vec = vectorizer_finally.transform(df_test['test_tweet'])\n",
    "df_test['sentiment_score'] = scaler.transform(df_test[['sentiment_score']])\n",
    "\n",
    "# Fuse Sentiment using offset method\n",
    "X_test_fused = X_test_vec.multiply(1 + abs(df_test['sentiment_score'].values.reshape(-1, 1)))\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_fused)\n",
    "\n",
    "# Display Predictions\n",
    "for text, pred in zip(df_test['test_tweet'], predictions):\n",
    "    sentiment = 'Positive' if pred == 1 else 'Negative'\n",
    "    print(f\"{text} ➡️ {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! As expected now the model is predicting both the tweets correctly. However, we did realise something. By adding a abs to the sentiment scores, it would not be able to classify phrases like, 'I am so fucking happy for you' correctly as the term 'fucking' will dominate this sentence and even though the sentiment score will be a positive it will turn out to be negative. Early in the project, we did say by adding sentiment score we will be able to tackle such cases, but we did not realise that such cases need to consider the words in the sentences. Meaning, for the sentence, 'I am so fucking happy for you', the sentiment score should penalise the word 'fucking' and 'boost' the word happy. Then the output will be as what we expect. But before doing this, we will first check if the model can classify, 'I am so fucking happy for you' correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"He is so fucking annoying\", \"I am so fucking happy for you\"]\n",
    "df_test = pd.DataFrame({'test_tweet': texts})\n",
    "df_test['sentiment_score'] = df_test['test_tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "X_test_vec = vectorizer_finally.transform(df_test['test_tweet'])\n",
    "df_test['sentiment_score'] = scaler.transform(df_test[['sentiment_score']])\n",
    "\n",
    "# Fuse Sentiment using offset method\n",
    "X_test_fused = X_test_vec.multiply(1 + abs(df_test['sentiment_score'].values.reshape(-1, 1)))\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_fused)\n",
    "\n",
    "# Display Predictions\n",
    "for text, pred in zip(df_test['test_tweet'], predictions):\n",
    "    sentiment = 'Positive' if pred == 1 else 'Negative'\n",
    "    print(f\"{text} ➡️ {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the the second tweet was being classified wrongly. Hence, an alternative approach has to be taken to solve this issue. As explained earlier firstly it is about assigning the appropriate boost and suppress effects to the words in the sentence. After brainstorming we decided to do it this way.\n",
    "\n",
    "Firstly we will calculate the sentiment score of the tweets from the dataset. Then we will calculate the sentiments of each of the words from the sentence. Following this it will be a bunch if conditional statements.\n",
    "\n",
    "- If the sentence sentiment score is positive & word sentiment score is positive (We will boost the overall word score) -> Forcing it to be a positive value\n",
    "- If the sentence sentiment score is negative & word sentiment score is negative (We will suppress the overall word score) -> Forcing it to be a negative value\n",
    "- If the sentence sentiment score is positive & word sentiment score is negative (We will heavily penalize the word score) -> While still keeping it negative\n",
    "- If the sentence sentiment score is negative & word sentiment score is positive (We will heavily penalize the word score) -> While still keeping it positive\n",
    "\n",
    " Moreover, by doing this, the predictions will heavily rely on the features and their adjusted score. The random forest model will be able to understand the different conditions and associate them with the correct classification based on the dataset. \n",
    "\n",
    " Furthermore, it is important to know that by adding an abs is a artificial fix and is not ideal. This is because it turns negative scores to positive and the random forest model will have difficulty differentiating between positive words and negative words in the sentence causing the accuracy of thr model to drop drastically. The ideal learning conditions for the model should be such that negative words remain as negative score & Positive words remain as positive score. The only difference is about boosting and suppressing them accordingly based on the sentence's sentiment score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we started to implement this word level sentiment, we came across a situation where the word 'fucking' was actually considered as a neutral word. The word 'fuck' was considered negative but not 'fucking'. These are rare occasions and and we are glad we were able to test out such a case. Hence, we manually gave a negative score to 'fucking' and added it into the VADER vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sia_booster = sia.constants.BOOSTER_DICT\n",
    "\n",
    "if 'fucking' in sia_booster:\n",
    "    sia_booster.pop('fucking')\n",
    "    print(\"Word 'fucking' DELETED from VADER\")\n",
    "\n",
    "# Step 3: Inject Permanent Custom Score\n",
    "sia.lexicon['fucking'] = -2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start with the conditional statements to boost and suppress the words scores. Here, we would not need to scale these scores as we want them to be reflective of what we explained earlier so that we would be able to maintain the intensity applied to the different words on case to case basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust word scores and return word-level scores\n",
    "def adjust_word_scores(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    sentence_score = sia.polarity_scores(sentence)['compound']\n",
    "    word_scores = [sia.polarity_scores(w)['compound'] for w in words]\n",
    "    \n",
    "    adjusted_scores = []\n",
    "    for word_score in word_scores:\n",
    "        if sentence_score > 0:\n",
    "            # Sentence is positive\n",
    "            if word_score > 0:\n",
    "                # Boost positive words a bit (e.g. ×3)\n",
    "                boosted = word_score * 3\n",
    "                if boosted < 0.1:  # ensure it's not too tiny\n",
    "                    boosted = 0.1\n",
    "                adjusted_scores.append(boosted)\n",
    "            elif word_score < 0:\n",
    "                # Keep negative words negative (or make them more negative).\n",
    "                # multiply by 1 => stays negative, or by 2 => more negative\n",
    "                # But not so large that it flips the entire sentence\n",
    "                more_neg = word_score * 1  # e.g., -0.3 stays -0.3\n",
    "                adjusted_scores.append(more_neg)\n",
    "            else:\n",
    "                # Neutral word => near zero\n",
    "                adjusted_scores.append(0.0)\n",
    "        else:\n",
    "            # Sentence is negative or neutral\n",
    "            if word_score < 0:\n",
    "                # Boost negative words a bit\n",
    "                more_neg = word_score * 3  # e.g., -0.2 => -0.6\n",
    "                if more_neg > -0.1:  # ensure it's below -0.1\n",
    "                    more_neg = -0.1\n",
    "                adjusted_scores.append(more_neg)\n",
    "            elif word_score > 0:\n",
    "                # Keep positive words positive but heavily suppress them\n",
    "                suppressed = word_score * 0.2  # e.g. 0.5 => 0.1\n",
    "                adjusted_scores.append(suppressed)\n",
    "            else:\n",
    "                # Neutral word => near zero\n",
    "                adjusted_scores.append(0.0)\n",
    "                \n",
    "    return np.array(adjusted_scores) if len(adjusted_scores) > 0 else np.array([0.0])\n",
    "\n",
    "# Load dataset\n",
    "df_balanced = pd.read_csv('train_balanced.csv')\n",
    "\n",
    "# Adjust sentiment scores for the training data (word-level)\n",
    "df_balanced['adjusted_word_scores'] = df_balanced['cleaned_tweet'].apply(lambda x: adjust_word_scores(x))\n",
    "\n",
    "# Vectorize the tweets\n",
    "vectorizer_final = TfidfVectorizer()\n",
    "X = vectorizer_final.fit_transform(df_balanced['cleaned_tweet'])\n",
    "X = csr_matrix(X)\n",
    "\n",
    "# Fuse Sentiment using word-level fusion\n",
    "X_fused = X.copy()\n",
    "row_indices, col_indices = X.nonzero()\n",
    "feature_names = vectorizer_final.get_feature_names_out().tolist()\n",
    "\n",
    "for i, (row, col) in enumerate(zip(row_indices, col_indices)):\n",
    "    word = feature_names[col]\n",
    "    sentence = df_balanced['cleaned_tweet'].iloc[row]\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    if word in words:\n",
    "        word_idx = words.index(word)\n",
    "        try:\n",
    "            X_fused[row, col] *= (1 + df_balanced['adjusted_word_scores'].iloc[row][word_idx])\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Testing\n",
    "texts = [\"He is so fucking annoying\", \"I am so fucking happy for you\"]\n",
    "df_test = pd.DataFrame({'test_tweet': texts})\n",
    "\n",
    "# Calculate adjusted sentiment scores for test data (word-level)\n",
    "df_test['adjusted_word_scores'] = df_test['test_tweet'].apply(lambda x: adjust_word_scores(x))\n",
    "\n",
    "X_test_vec = vectorizer_final.transform(df_test['test_tweet'])\n",
    "X_test_vec = csr_matrix(X_test_vec)\n",
    "X_test_fused = X_test_vec.copy()\n",
    "row_indices, col_indices = X_test_vec.nonzero()\n",
    "\n",
    "for i, (row, col) in enumerate(zip(row_indices, col_indices)):\n",
    "    word = feature_names[col]\n",
    "    sentence = df_test['test_tweet'].iloc[row]\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    if word in words:\n",
    "        word_idx = words.index(word)\n",
    "        try:\n",
    "            X_test_fused[row, col] *= (1 + df_test['adjusted_word_scores'].iloc[row][word_idx])\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "# Print X_test_fused with feature names and their fused scores\n",
    "X_test_fused_array = X_test_fused.toarray()\n",
    "\n",
    "print(\"\\nFeature Names and their fused scores:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"\\nText: {text}\")\n",
    "    for j, feature_name in enumerate(feature_names):\n",
    "        if X_test_fused_array[i, j] != 0:\n",
    "            print(f\"Feature: {feature_name}, Score: {X_test_fused_array[i, j]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuned Random Forest Model with updated sentiment score integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "df_balanced = pd.read_csv(\"train_balanced.csv\")\n",
    "\n",
    "# 2. Adjust sentiment scores\n",
    "df_balanced['adjusted_word_scores'] = df_balanced['cleaned_tweet'].apply(adjust_word_scores)\n",
    "\n",
    "# 3. Vectorize\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "X = csr_matrix(X)\n",
    "\n",
    "# 4. Fuse\n",
    "X_fused = X.copy()\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "row_indices, col_indices = X.nonzero()\n",
    "\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    word = feature_names[col]\n",
    "    words_in_sentence = df_balanced['cleaned_tweet'].iloc[row].lower().split()\n",
    "    if word in words_in_sentence:\n",
    "        word_idx = words_in_sentence.index(word)\n",
    "        try:\n",
    "            score = df_balanced['adjusted_word_scores'].iloc[row][word_idx]\n",
    "            X_fused[row, col] *= (1 + score)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "# 5. Train a normal random forest\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train Accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "# 6. Predict on new tweets\n",
    "test_texts = [\"He is so fucking annoying\", \"I am so fucking happy for you\"]\n",
    "df_test = pd.DataFrame({\"text\": test_texts})\n",
    "df_test[\"adjusted_word_scores\"] = df_test[\"text\"].apply(adjust_word_scores)\n",
    "\n",
    "X_test_vec = vectorizer.transform(df_test[\"text\"])\n",
    "X_test_vec = csr_matrix(X_test_vec)\n",
    "X_test_fused = X_test_vec.copy()\n",
    "\n",
    "row_indices, col_indices = X_test_vec.nonzero()\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    word = feature_names[col]\n",
    "    words_in_sentence = df_test[\"text\"].iloc[row].lower().split()\n",
    "    if word in words_in_sentence:\n",
    "        idx = words_in_sentence.index(word)\n",
    "        try:\n",
    "            score = df_test['adjusted_word_scores'].iloc[row][idx]\n",
    "            X_test_fused[row, col] *= (1 + score)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "preds = model.predict(X_test_fused)\n",
    "print(\"Predictions:\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! Managed to get the desired output. Now the model is able to classify both the tweets correctly. \n",
    "\n",
    "Cases like 'He is so fucking annoying', 'I am so happy', 'I am so fucking happy for you' will now be classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on entire test set:\", test_accuracy)\n",
    "\n",
    "# Optionally, print a classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in Optimised Random Forest\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('RF-Tuned-Sentiment.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('RF-Tuned-Sentiment.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"logisticregression.png\" alt=\"Image Description\" width=\"500\" style=\"border-radius: 10px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  \n",
    "  <div style=\"margin-right: auto;\">\n",
    "  <p> As mentioned in the early stage of the project, we will test out the logistic regression model with the same steps taken for the random forest model and see how well it performs now. </p>\n",
    "    <p>We will now explore how logistic regression model functions in this classification task to better understand the behind the scenes of the training process.\n",
    "</p>\n",
    "  <p>Logistic Regression is a popular supervised learning method for classification tasks. It computes a linear combination of features and applies the logistic (sigmoid) function to produce a probability score between 0 and 1. By setting a threshold (commonly 0.5), it assigns class labels (e.g., “positive” or “negative”). The model is trained by minimizing the log loss (cross-entropy), which heavily penalizes confident wrong predictions. This process naturally provides a probability output for each class, making logistic regression easy to interpret and a strong baseline in many scenarios. </p>  \n",
    "  <br>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuned Logistic regression Model with updated sentiment score integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "df_balanced = pd.read_csv(\"train_balanced.csv\")\n",
    "\n",
    "# 2. Adjust sentiment scores\n",
    "df_balanced['adjusted_word_scores'] = df_balanced['cleaned_tweet'].apply(adjust_word_scores)\n",
    "\n",
    "# 3. Vectorize\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "X = csr_matrix(X)\n",
    "\n",
    "# 4. Fuse\n",
    "X_fused = X.copy()\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "row_indices, col_indices = X.nonzero()\n",
    "\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    word = feature_names[col]\n",
    "    words_in_sentence = df_balanced['cleaned_tweet'].iloc[row].lower().split()\n",
    "    if word in words_in_sentence:\n",
    "        word_idx = words_in_sentence.index(word)\n",
    "        try:\n",
    "            score = df_balanced['adjusted_word_scores'].iloc[row][word_idx]\n",
    "            X_fused[row, col] *= (1 + score)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "# 5. Train a normal random forest\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(C = 1, class_weight= 'balanced', max_iter= 11000, penalty= 'l1', solver= 'liblinear') # Need update params\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train Accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "# 6. Predict on new tweets\n",
    "test_texts = [\"He is so fucking annoying\", \"I am so fucking happy for you\"]\n",
    "df_test = pd.DataFrame({\"text\": test_texts})\n",
    "df_test[\"adjusted_word_scores\"] = df_test[\"text\"].apply(adjust_word_scores)\n",
    "\n",
    "X_test_vec = vectorizer.transform(df_test[\"text\"])\n",
    "X_test_vec = csr_matrix(X_test_vec)\n",
    "X_test_fused = X_test_vec.copy()\n",
    "\n",
    "row_indices, col_indices = X_test_vec.nonzero()\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    word = feature_names[col]\n",
    "    words_in_sentence = df_test[\"text\"].iloc[row].lower().split()\n",
    "    if word in words_in_sentence:\n",
    "        idx = words_in_sentence.index(word)\n",
    "        try:\n",
    "            score = df_test['adjusted_word_scores'].iloc[row][idx]\n",
    "            X_test_fused[row, col] *= (1 + score)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "preds = model.predict(X_test_fused)\n",
    "print(\"Predictions:\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model was not able to predict the tweets correctly like the random forest models. This might be due to how logistic rgeression model functions.\n",
    "\n",
    "Logistic Regression uses a single linear boundary to separate classes. If certain words like “fucking” or “annoying” appear in mixed contexts in the training data, the model’s linear boundary may not push these words strongly into the “negative” side. Consequently, when it sees a new tweet containing those words, it might not interpret them as strongly negative—especially if there are other, more neutral or even slightly positive signals in the tweet.\n",
    "\n",
    "Random Forest, in contrast, builds multiple decision trees, each making smaller, more localized splits. Some trees will likely isolate the words “fucking” or “annoying” as strong indicators of negativity. Even if other trees misclassify, the majority vote can still label the tweet as negative. This ensemble approach is more flexible in capturing the nuanced signals that strongly indicate negativity for certain words.\n",
    "\n",
    "Hence, Random Forest outperforms logistic regression in this sentiment analysis task because it can more effectively learn from contradictory or partially neutral signals around words like “fucking” and “annoying,” and ultimately classify them as negative when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on entire test set:\", test_accuracy)\n",
    "\n",
    "# Optionally, print a classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in Optimised LR\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('LR-Tuned-Sentiment.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('SVM-Tuned-Sentiment.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <img src=\"svm.png\" alt=\"Image Description\" width=\"500\" style=\"border-radius: 10px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "  \n",
    "  <div style=\"margin-left: auto;\">\n",
    "    <p>Let's now explore how SVM model functions in a classifying task to better understand the behind the scenes of the training process.\n",
    "</p>\n",
    "    <p>SVM tries to separate classes with the widest possible margin. If the data isnt linearly separable, kernel functions transform them into a higher-dimensional space. The key aspect of a SVM model is its focus on the support vectors. These support vectors are the critical training samples that define the boundary which determines the accuracy of the classification. They lie closest to the hyperplanes.</p>  \n",
    "    Example:\n",
    "    <br>\n",
    "    <p>Ultimately, SVM tries to find a straight line (2 dimensional) or a hyperplane (in higher dimensions) that perfectly separates the classes (target). Among all the possible separating hyperplanes, SVM chooses the one that maximises the margin (is less sensitive to small changes or noise in the data). By maximizing the margin, SVM aims to generalize better.</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuned SVM Model with updated sentiment score integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "df_balanced = pd.read_csv(\"train_balanced.csv\")\n",
    "\n",
    "# 2. Adjust sentiment scores\n",
    "df_balanced['adjusted_word_scores'] = df_balanced['cleaned_tweet'].apply(adjust_word_scores)\n",
    "\n",
    "# 3. Vectorize\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "X = csr_matrix(X)\n",
    "\n",
    "# 4. Fuse\n",
    "X_fused = X.copy()\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "row_indices, col_indices = X.nonzero()\n",
    "\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    word = feature_names[col]\n",
    "    words_in_sentence = df_balanced['cleaned_tweet'].iloc[row].lower().split()\n",
    "    if word in words_in_sentence:\n",
    "        word_idx = words_in_sentence.index(word)\n",
    "        try:\n",
    "            score = df_balanced['adjusted_word_scores'].iloc[row][word_idx]\n",
    "            X_fused[row, col] *= (1 + score)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "# 5. Train a normal random forest\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train Accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "# 6. Predict on new tweets\n",
    "test_texts = [\"He is so fucking annoying\", \"I am so fucking happy for you\"]\n",
    "df_test = pd.DataFrame({\"text\": test_texts})\n",
    "df_test[\"adjusted_word_scores\"] = df_test[\"text\"].apply(adjust_word_scores)\n",
    "\n",
    "X_test_vec = vectorizer.transform(df_test[\"text\"])\n",
    "X_test_vec = csr_matrix(X_test_vec)\n",
    "X_test_fused = X_test_vec.copy()\n",
    "\n",
    "row_indices, col_indices = X_test_vec.nonzero()\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    word = feature_names[col]\n",
    "    words_in_sentence = df_test[\"text\"].iloc[row].lower().split()\n",
    "    if word in words_in_sentence:\n",
    "        idx = words_in_sentence.index(word)\n",
    "        try:\n",
    "            score = df_test['adjusted_word_scores'].iloc[row][idx]\n",
    "            X_test_fused[row, col] *= (1 + score)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "preds = model.predict(X_test_fused)\n",
    "print(\"Predictions:\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model was not able to predict the 2 tweets correctly unlike the random forest model. This might be due to the way SVM functions. This is a similar case of comparison between random forest and logistic regression.\n",
    "\n",
    "An SVM seeks a single hyperplane (or set of hyperplanes in kernel space) that separates all positive and negative samples with maximum margin. Even after integrating sentiment scores, possibly certain features (like “fucking,” “annoying”) don’t consistently appear in strongly negative samples during training—or if they appear in mixed/neutral contexts—the SVM might not push them far into the “negative” side of the boundary.\n",
    "\n",
    "However, a random Forest model consists of many decision trees. Each tree makes smaller, local splits, some of which can strongly isolate “fucking” + “annoying” as negative signals. Even if some trees misclassify, others might correctly label those words as negative, leading to a majority “negative” vote overall.\n",
    "\n",
    "Therefore, random forest would be a better fit for this sentiment analysis task on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on entire test set:\", test_accuracy)\n",
    "\n",
    "# Optionally, print a classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified tweets in Optimised SVM\n",
    "misclassified_indices = y_test[y_test != y_pred].index\n",
    "misclassified_tweets = df_balanced.loc[misclassified_indices, 'cleaned_tweet']\n",
    "actual_classes = y_test[y_test != y_pred]\n",
    "predicted_classes = y_pred[y_test != y_pred]\n",
    "\n",
    "# Print the misclassified tweets along with their actual and predicted classes\n",
    "for tweet, actual, predicted in zip(misclassified_tweets, actual_classes, predicted_classes):\n",
    "    print(f\"Tweet: {tweet}\\nActual Class: {actual}\\nPredicted Class: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sys\n",
    "size = sys.getsizeof(model)\n",
    "print(f\"Model memory usage (shallow): {size} bytes\")\n",
    "# From Pympler\n",
    "total_size = asizeof.asizeof(model)\n",
    "print(f\"Model memory usage (deep): {total_size} bytes\")\n",
    "# Pickle Model Dump\n",
    "with open('SVM-Tuned-Sentiment.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "file_size = os.path.getsize('SVM-Tuned-Sentiment.pkl')\n",
    "print(f\"Serialized model file size: {file_size} bytes\")\n",
    "\n",
    "# Timer\n",
    "start_time = time.perf_counter()  \n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.perf_counter()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; /* Smaller padding */\n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; /* Smaller Font */\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; /* Thinner Border */\n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; /* Smaller Width */\n",
    "              margin: auto;\">8 |\n",
    "  Final Evaluation: Random Forest Vs SVM\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Why Random Forest And SVM Misclassify Tweets</h3>\n",
    "Random Forest (Ambiguous or Sarcastic Language)\n",
    "Random Forests often rely on straightforward keyword splits in their individual decision trees. When tweets contain ambiguous, sarcastic, or context-dependent language, the model’s if–else splits can be misled. In other words, certain words might appear “hateful” or “negative” out of context, leading the ensemble to vote incorrectly.\n",
    "\n",
    "SVM (Conflicting Feature Weights)\n",
    "An SVM determines class boundaries based on how strongly each token (feature) weighs toward one class or another. If some words appear in both hateful and non-hateful contexts in the training data—or if the SVM does not assign strong enough or consistent enough weights to critical tokens—borderline tweets can end up on the wrong side of the decision boundary. This is especially true when language usage is inconsistent or when context is subtle.\n",
    "\n",
    "<h3>Comparing the Misclassification</h3>\n",
    "When comparing tweets that got misclassified by Random Forest to SVM,\n",
    "\n",
    "Random Forest tend misclassify tweets with specific words that strongly correlate with one class in the training set. Even if the overall context is contradictory, the presence of certain trigger words can cause a majority of trees to vote incorrectly. Tweets that have a lot of rare or slang words that appear noisy to the model. A few trees might overfit to these words and sway the vote.\n",
    "\n",
    "Whereas for SVM, it tends to misclassify tweets with more subtle context or borderline language. If the tweet’s feature vector places it close to the decision boundary, small contextual cues might not be enough to tip it into the correct class. Tweets where certain offensive words appear in multiple contexts in training. If the SVM can’t draw a clean boundary due to contradictory examples, it may misclassify some borderline cases.\n",
    "\n",
    "<h3>Why They Were Misclassified Differently</h3>\n",
    "\n",
    "Random Forest model misclassifies tweets when it encounters language that is highly context-dependent sarcasm or ambiguous wording. Individual decision trees latch onto certain keywords, but without deeper context, they may vote incorrectly—especially if the ambiguous phrasing was not well represented in the training data.\n",
    "\n",
    "The SVM tends to misclassify tweets whose language contains mixed signals such as a slur used in a non-hateful context, or negative words used ironically. Because the SVM’s decision boundary is based on how strongly each token correlates with a class, inconsistent usage or conflicting feature patterns can push borderline tweets onto the wrong side of the hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                                 | Accuracy | Misclassified | Memory (Shallow) | Memory (Deep) | File Size     | Training Time |\n",
    "|---------------------------------------|---------:|-------------:|-----------------:|--------------:|--------------:|--------------:|\n",
    "| **RF clean w/o tuning**               | 0.9452   | 89           | 48 bytes         | 2352 bytes    | 649 bytes     | 4.3647 s      |\n",
    "| **RF clean w tuning**                 | 0.9470   | 86           | 48 bytes         | 2352 bytes    | 650 bytes     | 12.5593 s     |\n",
    "| **RF clean w tuning & Sentiment**     | 0.9501   | 81           | 48 bytes         | 359720 bytes  | 65359759 bytes| 14.4208 s     |\n",
    "| **SVM clean w/o tuning**              | 0.9323   | 110          | 48 bytes         | 1640 bytes    | 346 bytes     | 1.5495 s      |\n",
    "| **SVM clean w tuning**                | 0.9427   | 93           | 48 bytes         | 1648 bytes    | 342 bytes     | 0.7535 s      |\n",
    "| **SVM clean w tuning & Sentiment**    | 0.9119   | 143          | 48 bytes         | 1358392 bytes | 694984 bytes  | 1.7863 s      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest</h3>\n",
    "Intially the random forest model which did not use any hyperparameters misclassified 88 tweets when it was tested against the test set. However, when the model was trained with the hyperparameters attained from the grid search, the accuracy improved and the number of tweets misclassfied dropped by 3. This indicates that the hyperparameter tuning helped improve the model. Then when we integrated in the sentiment score we were able to see that it had the highest accuracy score of 95.01%. It also missclassified the least tweets number of tweets amongst all the different model. \n",
    "\n",
    "As expected the memory size and the file size was the largest for the fine tuned model with sentiment intergation. However, it was able to train at a faster pace compared to the model which did not have sentiment score into play.\n",
    "Comparing models with and without hyperparameter tuning revealed that the primary distinction between them was training time. As expected, the model which used the hyperparameters took a longer time to train. Ultimately, the random forest model that incorporated the sentiment score delivered the highest accuracy while still maintaining a reasonable training duration.\n",
    "\n",
    "<h3>SVM</h3>\n",
    "The SVM model without any hyperparameter tuning achieved an accuracy of 93.23% on the test set, misclassifying 110 tweets. After applying hyperparameter tuning, its accuracy improved to 94.27%, reducing the number of misclassified tweets to 93. This confirms that tuning helped refine the decision boundary for the SVM, resulting in fewer errors.\n",
    "\n",
    "However, when we integrated the sentiment score into the SVM, the accuracy dropped to 91.19%, with 143 tweets misclassified—its worst performance among all SVM variants. Interestingly, this model had a large increase in deep memory usage and file size compared to the untuned versions, yet the training time remained relatively short at 1.79 seconds. This suggests that although sentiment integration significantly expanded the model’s feature space (increasing memory/file size), it did not translate into improved predictive performance for SVM in this setup.\n",
    "\n",
    "<h3>Random Forest Vs SVM</h3>\n",
    "--Accuracy and Misclassifications--\n",
    "<br>\n",
    "Random Forest with sentiment integration emerged as the best performer overall (95.01% accuracy, 81 misclassifications).\n",
    "SVM peaked at 94.27% with tuning (93 misclassifications) but fell to 91.19% when sentiment was introduced.\n",
    "This indicates that Random Forest leveraged the additional sentiment features more effectively than SVM.\n",
    "<br>\n",
    "<br>\n",
    "--Training Time--\n",
    "<br>\n",
    "Random Forest generally took longer to train than SVM, especially when hyperparameter tuning was involved (12.5593s vs. 0.7535s in the tuned variants).\n",
    "However, the sentiment-based Random Forest (14.4208s) trained only slightly longer than the non-sentiment tuned variant (12.5593s), suggesting that certain hyperparameters or data interactions might have influenced training speed. Also it is important to SVM training time was shorter when it made use of hyperparamters which is not the usual case as when tuned it usually takes a longer time. A possible reason for this is that the hyperparameters fit the data better and simplifed the optimization process (e.g., smaller C, simpler kernel, appropriate gamma). As a result, it converges more quickly than the default settings, which may have resulted in the faster training time.\n",
    "<br>\n",
    "<br>\n",
    "--Model Size and Memory--\n",
    "<br>\n",
    "Random Forest with sentiment scores incorporated had the largest file size (about 65-66 MB) and significantly higher deep memory usage compared to other models, reflecting the increased complexity of storing numerous trees and sentiment-augmented features.\n",
    "SVM with sentiment also saw an increase in memory and file size, though less pronounced than Random Forest, yet it failed to achieve improvements in accuracy.\n",
    "<br>\n",
    "<br>\n",
    "--Overall Winner--\n",
    "<br>\n",
    "For this dataset and approach, Random Forest with sentiment integration provided the highest accuracy (95.01%) and a decent training time (14.4208s).\n",
    "While SVM offered a faster training time and smaller file size for its best-tuned version, it never surpassed Random Forest in accuracy, especially once sentiment features were introduced. \n",
    "<br>\n",
    "This is likely because SVM tries to find a maximal margin in a  high-dimensional space. If a new feature-like a sentiment score with more noise or less predictive patterns is added, this will affect the margin calculated by the SVM. That one noisy feature can derail the SVM if it’s placed too much emphasis on it.\n",
    "However, a random forest averages over many decision trees, each trained on a bootstrapped subset of the data. If the sentiment score is useful only in some cases, some trees will learn to leverage it, while others effectively ignore it. This ensemble approach dilutes the negative impact of noise in that feature but still captures its positive signal if it’s there.\n",
    "Since our goal is to classify the tweets, the accuracy is a important factor to be considered as ulimately we want to detect the negative tweets and prevent the user from receiving them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; /* Smaller padding */\n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; /* Smaller Font */\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; /* Thinner Border */\n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; /* Smaller Width */\n",
    "              margin: auto;\">9 |\n",
    "  Conclusion\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px; align-items: center;\">\n",
    "  <div style=\"margin-right: auto;\">\n",
    "  <p>This entire journey has been fullfilling one.<p>\n",
    "\n",
    "  <p>Firstly, we started with the exploratory data analysis. This was done using pandas to perform text based analysis while also generating charts using various visualization libraries. Through this we got to attain a deeper understanding on the data set and the features we am working with for this particular NLP task. We gained valuable insights which played a huge part throughout this project.<p>\n",
    "\n",
    "  <p>Moving on, we started with the data preprocessing & data cleaning where we cleaned up the data and prepared it for feature engineering. Tasks included removing stopwords, removing unwanted words and punctuations.<p>\n",
    "\n",
    "  <p>We then started with feature engineering where we made use of the existing columns and generated valuable information which can be used to value add during the modelling. Additions included, stemming & lemmatization, oversampling & undersampling, addition of sentiment polarity scores using VADER, weights scoring through diagreement penalty method.<p>\n",
    "\n",
    "  <p>Now, we got to modelling where we streamlined different classification algorithms by using pipelines. With the classifications algorithms working hand in hand with the vectorizers, we were able to train and test out the different models. Conducted various experiments such as using weights during modelling and using different vectorizers for the modelling to make informed conclusions before moving onto the hyperparameter tuning. We also narrowed down to the most successful 2 models which were SVMs and Random forest models.<p>\n",
    "\n",
    "  <p>At this junction of the notebook, we made use of grid search to generate the most suitable hyperparameters for models in hopes of attaining a better accuracy. We then evaluated if the usage of hyperparameters were useful for the two different models. After evaluations, we decided moving on we will be making use of random forest models without tuning to allow its element of randomness to shine.<p>\n",
    "\n",
    "  <p>Now, we reached the portion we spent the most time on in this notebook. Re-evaluation & further enhancements. Here is where we tested out the models with sample tweets and evaluated the success of the models. We faced neumerous challenges attaining the desired prediction. It was like solving a crime scene with many loopholes. Every loophole allowed me to critically think of ways to overcome it. Some key features implemented here to attain the desired prediction was, usage of ngrams, VADER sentiment scores, offset fusion multiplier (VADER), lemmatized words + original words, individual word sentiment score comparison against sentence sentiment score, and custom random forest model with specific rule sets. These features collectively value added to attain the best performing model at the end of the notebook. <p>\n",
    "\n",
    "  <p>In a nutshell, we learnt a lot while working on this NLP task. Looking forward to more data science tasks in the future!<p>\n",
    "\n",
    "  <p>Thank you!<p>\n",
    "  </div>\n",
    "    <img src=\"smiley.png\" alt=\"Image Description\" width=\"400\" style=\"border-radius: 10px; box-shadow: 0 0 15px rgba(29,161,242,0.8);\">\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px; /* Smaller padding */\n",
    "              color:#1DA1F2; \n",
    "              font-size:150%; /* Smaller Font */\n",
    "              border-radius:20px; \n",
    "              border-width: 3px; /* Thinner Border */\n",
    "              border-style: solid; \n",
    "              border-color: #1DA1F2; \n",
    "              background-color:white; \n",
    "              font-weight:500; \n",
    "              letter-spacing: 1px; \n",
    "              text-shadow: 1px 1px 5px rgba(29,161,242,0.8); \n",
    "              box-shadow: 0 0 10px rgba(29,161,242,0.5); \n",
    "              display: inline-block; \n",
    "              width: auto; /* Smaller Width */\n",
    "              margin: auto;\">10 |\n",
    "  References\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image References\n",
    "1. [Emoji Sentiment Analysis](https://bluemoji.io/)\n",
    "2. [Stop Words](https://www.google.com/url?sa=i&url=https%3A%2F%2Fbotpenguin.com%2Fglossary%2Fstop-words&psig=AOvVaw3YrI2o4yu7NWi75-ZInAvq&ust=1741275339231000&source=images&cd=vfe&opi=89978449&ved=0CBAQjRxqFwoTCIiS_6Gi84sDFQAAAAAdAAAAABAE/)\n",
    "3. [Sentiment Score Distribution](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2F%40avicsebooks%2Fpart-18-ml-deep-learning-and-reinforcement-learning-468f7340aaf1&psig=AOvVaw29_Ie2acunssyWtbJ9SuTC&ust=1741275366654000&source=images&cd=vfe&opi=89978449&ved=0CBAQjRxqFwoTCLDs-66i84sDFQAAAAAdAAAAABAE)\n",
    "4. [Emotion Categories](https://www.google.com/url?sa=i&url=https%3A%2F%2Frozwarner.com%2F2017%2F08%2F12%2Fgoverning-from-the-gut-emotion-in-world-politics%2F&psig=AOvVaw14fjPF3KCEqKGPS8tEBEUz&ust=1741275400811000&source=images&cd=vfe&opi=89978449&ved=0CBAQjRxqFwoTCKiBtb-i84sDFQAAAAAdAAAAABAE)\n",
    "5. [VADER Sentiment Scoring Process](https://www.researchgate.net/figure/VADER-Sentiment-Scoring-Process_fig1_371247846)\n",
    "6. [Class Balancing Techniques](https://www.researchgate.net/figure/Class-balancing-random-undersampling-and-random-oversampling_fig4_360676832)\n",
    "7. [Lemmatization](https://botpenguin.com/glossary/lemmatization)\n",
    "8. [Stemming Process](https://www.google.com/url?sa=i&url=https%3A%2F%2Fbotpenguin.com%2Fglossary%2Fstemming&psig=AOvVaw1s-cIIDHIwbE13L2Vb-yqx&ust=1741275424725000&source=images&cd=vfe&opi=89978449&ved=0CBAQjRxqFwoTCND71sqi84sDFQAAAAAdAAAAABAE)\n",
    "9. [Twitter Blue](https://www.digitaltrends.com/social-media/what-is-twitter-blue-and-is-it-worth-it/)\n",
    "10. [Hate Speech Word Cloud](https://www.ukposters.co.uk/hate-speech-word-cloud-on-blue-background-f774661063)\n",
    "11. [SVM](https://vitalflux.com/wp-content/uploads/2022/08/support-vector-machine-1-1280x720.png)\n",
    "12. [Logistic Regression](https://images.spiceworks.com/wp-content/uploads/2022/04/11040521/46-4-e1715636469361.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### Notebook References\n",
    "1. [Twitter Sentiment Extraction - EDA & Model](https://www.kaggle.com/code/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model)\n",
    "2. [Hate Speech Detection (English)](https://www.kaggle.com/code/minthihatun/hate-speech-detection-english)\n",
    "3. [Beginner Guide for Sentiment Analysis (95% Accuracy)](https://www.kaggle.com/code/ppsheth91/beginner-guide-for-sentiment-analysis-95-accuracy)\n",
    "4. [Predicting Hate Speech using Machine Learning](https://www.kaggle.com/code/abuchionwuegbusi/predicting-hate-speech-using-machine-learningg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
