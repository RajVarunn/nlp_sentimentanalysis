{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Natural Language Processing (NLP) Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display_html\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.tree import plot_tree\n",
    "import plotly.express as px\n",
    "\n",
    "# Data Manipulation and Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from pympler import asizeof\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "class_labels = {0: 'Hate Speech', 1: 'Offensive Language', 2: 'Neither'}\n",
    "class_counts = df['class'].value_counts()\n",
    "df['labels'] = df['class'].map(class_labels)\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "df['class'] = df['class'].apply(lambda x: 0 if x in [0, 1] else 1)\n",
    "\n",
    "class_labels = {0: 'negative', 1: 'positive'}\n",
    "df['labels'] = df['class'].map(class_labels)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('not')  # Keeping 'not' as it is important in negation (Can change the meaning of the sentence)\n",
    "\n",
    "def clean(text):\n",
    "    # Remove 'RT' (Retweet indicator)\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    # Remove URLs \n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove Twitter handles \n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    # Removing the stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Rejoin the words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "df.loc[:, 'cleaned_tweet'] = df['tweet'].apply(clean)\n",
    "\n",
    "tweet_length = df['cleaned_tweet'].apply(len)\n",
    "df['tweet_length'] = df['cleaned_tweet'].apply(len)\n",
    "Q1 = tweet_length.quantile(0.25)\n",
    "Q3 = tweet_length.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df = df[(tweet_length >= lower_bound) & (tweet_length <= upper_bound)]\n",
    "\n",
    "df.drop(['tweet', 'tweet_length'], axis=1, inplace=True)\n",
    "\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna()\n",
    "\n",
    "df.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/rajvarun/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rajvarun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/rajvarun/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'fucking' DELETED from VADER\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, get_wordnet_pos(token))\n",
    "        lemmatized_tokens.append(token)  \n",
    "        if lemma != token:\n",
    "            lemmatized_tokens.append(lemma)  \n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "final_df = pd.read_csv('cleaned_dataset.csv')\n",
    "final_df = final_df.dropna()\n",
    "final_df['cleaned_tweet'] = final_df['cleaned_tweet'].apply(lemmatize_text) \n",
    "\n",
    "df_majority=final_df[(final_df['class']==0)] \n",
    "df_minority=final_df[(final_df['class']==1)] \n",
    "df_majority_undersample=resample(df_majority, replace=False, n_samples=df_minority.shape[0], random_state=42)\n",
    "df_balanced = pd.concat([df_majority_undersample, df_minority])\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def get_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "sia_booster = sia.constants.BOOSTER_DICT\n",
    "if 'fucking' in sia_booster:\n",
    "    sia_booster.pop('fucking')\n",
    "    print(\"Word 'fucking' DELETED from VADER\")\n",
    "sia.lexicon['fucking'] = -2.0\n",
    "\n",
    "df_balanced['sentiment_score'] = df_balanced['cleaned_tweet'].apply(get_sentiment)\n",
    "df_balanced.drop(['count', 'hate_speech_count', 'offensive_language_count', 'neither_count'], axis=1, inplace=True)\n",
    "df_balanced = df_balanced[df_balanced['cleaned_tweet'].str.strip() != '']\n",
    "df_balanced.to_csv('train_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9995378927911276\n",
      "Test Accuracy: 0.9532019704433498\n",
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "def adjust_word_scores(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    sentence_score = sia.polarity_scores(sentence)['compound']\n",
    "    word_scores = [sia.polarity_scores(w)['compound'] for w in words]\n",
    "    \n",
    "    adjusted_scores = []\n",
    "    for word_score in word_scores:\n",
    "        if sentence_score > 0:\n",
    "            if word_score > 0:\n",
    "                boosted = word_score * 3\n",
    "                if boosted < 0.1: \n",
    "                    boosted = 0.1\n",
    "                adjusted_scores.append(boosted)\n",
    "            elif word_score < 0:\n",
    "                more_neg = word_score * 1  \n",
    "                adjusted_scores.append(more_neg)\n",
    "            else:\n",
    "                adjusted_scores.append(0.0)\n",
    "        else:\n",
    "            if word_score < 0:\n",
    "                more_neg = word_score * 3 \n",
    "                if more_neg > -0.1: \n",
    "                    more_neg = -0.1\n",
    "                adjusted_scores.append(more_neg)\n",
    "            elif word_score > 0:\n",
    "                suppressed = word_score * 0.2\n",
    "                adjusted_scores.append(suppressed)\n",
    "            else:\n",
    "                adjusted_scores.append(0.0)\n",
    "                \n",
    "    return np.array(adjusted_scores) if len(adjusted_scores) > 0 else np.array([0.0])\n",
    "\n",
    "\n",
    "df_balanced = pd.read_csv(\"train_balanced.csv\")\n",
    "df_balanced['adjusted_word_scores'] = df_balanced['cleaned_tweet'].apply(adjust_word_scores)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_balanced['cleaned_tweet'])\n",
    "X = csr_matrix(X)\n",
    "\n",
    "X_fused = X.copy()\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "row_indices, col_indices = X.nonzero()\n",
    "\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    word = feature_names[col]\n",
    "    words_in_sentence = df_balanced['cleaned_tweet'].iloc[row].lower().split()\n",
    "    if word in words_in_sentence:\n",
    "        word_idx = words_in_sentence.index(word)\n",
    "        try:\n",
    "            score = df_balanced['adjusted_word_scores'].iloc[row][word_idx]\n",
    "            X_fused[row, col] *= (1 + score)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "y = df_balanced['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split= 2, n_estimators= 300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train Accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "test_texts = [\"He is so fucking annoying\", \"I am so fucking happy for you\"]\n",
    "df_test = pd.DataFrame({\"text\": test_texts})\n",
    "df_test[\"adjusted_word_scores\"] = df_test[\"text\"].apply(adjust_word_scores)\n",
    "\n",
    "X_test_vec = vectorizer.transform(df_test[\"text\"])\n",
    "X_test_vec = csr_matrix(X_test_vec)\n",
    "X_test_fused = X_test_vec.copy()\n",
    "\n",
    "row_indices, col_indices = X_test_vec.nonzero()\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    word = feature_names[col]\n",
    "    words_in_sentence = df_test[\"text\"].iloc[row].lower().split()\n",
    "    if word in words_in_sentence:\n",
    "        idx = words_in_sentence.index(word)\n",
    "        try:\n",
    "            score = df_test['adjusted_word_scores'].iloc[row][idx]\n",
    "            X_test_fused[row, col] *= (1 + score)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "preds = model.predict(X_test_fused)\n",
    "print(\"Predictions:\", preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
